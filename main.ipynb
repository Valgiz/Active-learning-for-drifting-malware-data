{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as ltb\n",
    "from sklearn.datasets import load_iris\n",
    "from modAL.models import ActiveLearner\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "from modAL.uncertainty import entropy_sampling\n",
    "from modAL.uncertainty import margin_sampling\n",
    "from modAL.uncertainty import classifier_uncertainty\n",
    "from modAL.batch import uncertainty_batch_sampling\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import DataDriftTable\n",
    "from evidently.metrics import DatasetDriftMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install evidently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emu_l = pd.read_csv('../KronoDroid/emu_legitimate_v1.csv')\n",
    "print('emu legitimate done')\n",
    "df_emu_m = pd.read_csv('../KronoDroid/emu_malware_v1.csv')\n",
    "print('emu malware done')\n",
    "df_real_l = pd.read_csv('../KronoDroid/real_legitimate_v1.csv')\n",
    "print('real legitimate done')\n",
    "df_real_m = pd.read_csv('../KronoDroid/real_malware_v1.csv')\n",
    "print('real malware done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emu_l.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real_m.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real_l.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_real_m[\"HighestModDate\"][1])\n",
    "test = datetime.strptime(df_real_m[\"HighestModDate\"][1], '%m/%d/%Y') \n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_real_m.columns.difference(df_emu_m.columns)) \n",
    "\n",
    "print(df_real_m.columns.difference(df_real_l.columns))\n",
    "\n",
    "print(df_emu_l.columns.difference(df_emu_m.columns)) \n",
    "\n",
    "print(df_emu_l.columns.difference(df_real_l.columns)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarter(str_date):\n",
    "    Quarter = 0\n",
    "    nb_err = 0\n",
    "    try:\n",
    "        date = datetime.strptime(str_date, '%m/%d/%Y')\n",
    "    except :\n",
    "        nb_err+=1\n",
    "        print('///////////////////////err////////////////////////')\n",
    "        print(str_date)\n",
    "        date = datetime.strptime('1/1/1980', '%m/%d/%Y')\n",
    "    if (date.month<3):\n",
    "        Quarter = 1\n",
    "    elif (date.month>2 and date.month<5):\n",
    "        Quarter = 2\n",
    "    elif (date.month>4 and date.month<7):\n",
    "        Quarter = 3\n",
    "    elif (date.month>6 and date.month<9):\n",
    "        Quarter = 4\n",
    "    elif (date.month>8 and date.month<11):\n",
    "        Quarter = 5\n",
    "    elif (date.month>10):\n",
    "        Quarter = 6\n",
    "    return (Quarter, date.year)\n",
    "\n",
    "def get_years_list(date_list):\n",
    "    dfy = pd.DataFrame(columns=['year'])\n",
    "    for i, date in enumerate(date_list):\n",
    "        dfy.loc[i] = get_quarter(date)[1]\n",
    "    return dfy['year']\n",
    "\n",
    "def get_quarter_list(date_list):\n",
    "    dfqy = pd.DataFrame(columns=['quarter','year'])\n",
    "    for i, date in enumerate(date_list):\n",
    "        dfqy.loc[i] = get_quarter(date)\n",
    "    return dfqy\n",
    "\n",
    "def see_balance(l, pool_name='pool'):\n",
    "    nb_malware =0\n",
    "    for i in l:\n",
    "        if(i.all()):\n",
    "            nb_malware+=1\n",
    "            \n",
    "    print(\"malware in \" + pool_name + \": \" + str(nb_malware) +' ('+str(100*nb_malware/len(l))+'%)')\n",
    "    print(\"normal in \" + pool_name + \": \" + str(len(l) - nb_malware) +' ('+str(100*(len(l)-nb_malware)/len(l))+'%)')\n",
    "    return nb_malware\n",
    "\n",
    "def init_quarter_pool(data, quart, year, ratio = 0.25):\n",
    "    y = data.loc[:,'Malware'].to_numpy()\n",
    "    \n",
    "    a = get_quarter_list(data['HighestModDate'])\n",
    "    \n",
    "    x = data.drop(data[(a['quarter']!= quart) | (a['year']!= year)].index)\n",
    "    y = x.loc[:,'Malware'].to_numpy()\n",
    "    x = x.drop(columns=['Malware', 'HighestModDate']).to_numpy()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=ratio)\n",
    "    while(not see_balance(y_test)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=ratio)\n",
    "    return [X_train, X_test, y_train, y_test] \n",
    "\n",
    "#print(get_quarter(df[\"HighestModDate\"].iloc[1]))\n",
    "#print(get_years_list(df[\"HighestModDate\"][1:4]))\n",
    "#print(get_quarter_list(df[\"HighestModDate\"][1:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define time period and separate features\n",
    "full = 66\n",
    "temp_m3 = df_real_m.copy()\n",
    "temp_l3 = df_real_l.copy()\n",
    "\n",
    "temp_m3.drop(temp_m3.columns[469:], axis=1, inplace=True)\n",
    "temp_m3.drop(temp_m3.columns[464:468], axis=1, inplace=True)\n",
    "temp_m3.drop(temp_m3.columns[0], axis=1, inplace=True)\n",
    "\n",
    "temp_l3.drop(temp_l3.columns[469:], axis=1, inplace=True)\n",
    "temp_l3.drop(temp_l3.columns[464:468], axis=1, inplace=True)\n",
    "temp_l3.drop(temp_l3.columns[0], axis=1, inplace=True)\n",
    "\n",
    "df_hyb = pd.concat([temp_l3, temp_m3], ignore_index=True)\n",
    "\n",
    "df_hyb.drop(df_hyb[df_hyb['HighestModDate'] == '1980-00-00'].index, inplace = True)\n",
    "df_hyb.reset_index(inplace=True, drop=True)\n",
    "df_hyb.drop(df_hyb[get_years_list(df_hyb['HighestModDate']) >2023].index, inplace = True)\n",
    "df_hyb.reset_index(inplace=True, drop=True)\n",
    "\n",
    "quarter_list_hyb = []\n",
    "quarter_list_sys = []\n",
    "quarter_list_perm = []\n",
    "q1 = 0\n",
    "y1 = 2009\n",
    "\n",
    "for nb_quarter in range (full):\n",
    "    next_q = (q1+nb_quarter)%6+1\n",
    "    next_y = y1+(q1+nb_quarter)//6\n",
    "    print(next_q)\n",
    "    print(next_y)\n",
    "    quarter_list_hyb.append(init_quarter_pool(df_hyb, next_q ,next_y))\n",
    "    quarter_list_hyb[-1].append(next_q)\n",
    "    quarter_list_hyb[-1].append(next_y)\n",
    "\n",
    "for nb_quarter in range (len(quarter_list_hyb)):\n",
    "    X,x,L,l,q,y = quarter_list_hyb[nb_quarter]\n",
    "    \n",
    "    curr_perm_train = np.delete(X, slice(289),1)\n",
    "    curr_perm_test = np.delete(x, slice(289),1)\n",
    "    quarter_list_sys.append([curr_perm_train,curr_perm_test,L,l,q,y])\n",
    "    \n",
    "    curr_sys_train = np.delete(X, slice(289,462),1)\n",
    "    curr_sys_test = np.delete(x, slice(289,462),1)\n",
    "    quarter_list_perm.append([curr_sys_train,curr_sys_test,L,l,q,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(75489):\n",
    "    t = get_quarter(df.iloc[i,-1])\n",
    "    if(t[1]>2022):\n",
    "        print(t)\n",
    "        print(i)\n",
    "    l.append(t[1])\n",
    "min(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_y=2009\n",
    "current_q = 1\n",
    "total_l =0\n",
    "for e in quarter_list[12:]:\n",
    "    c = len(e[2])\n",
    "    total_l+=c\n",
    "    print('q='+str(e[4]))\n",
    "    print('y='+str(e[5]))\n",
    "    print(c)\n",
    "print(total_l)\n",
    "\n",
    "#print(quarter_list[4])\n",
    "#for i,e in enumerate(quarter_list):\n",
    "#    print(len(e[2]))\n",
    "#    see_balance(e[2])\n",
    "#    see_balance(e[3])\n",
    "    \n",
    "#for i in range(len(quarter_list)-1):\n",
    "#    it = current_q + 6*(current_y-2009)\n",
    "#    current_y = quarter_list[it][5]\n",
    "#    current_q = quarter_list[it][4]\n",
    "#    print(current_q)\n",
    "#    print(current_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df=df.drop(df[get_years_list(df['HighestModDate']) <2000].index)\n",
    "hist_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "l = []\n",
    "for i in range(len(hist_df.index)):\n",
    "    t = get_quarter(hist_df.iloc[i,-1])\n",
    "    if(t[1]>2022):\n",
    "        print(t)\n",
    "        print(i)\n",
    "    l.append(t[1])\n",
    "\n",
    "plt.hist(l, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_balanced_training_pool(x, y, size):\n",
    "    indices_l = np.where(y == 0)[0]\n",
    "    indices_m = np.where(y == 1)[0]\n",
    "    idl = np.random.choice(indices_l, int(size/2))\n",
    "    idm = np.random.choice(indices_m, size - int(size/2))\n",
    "    \n",
    "    x_train = np.concatenate((x[idl], x[idm]))\n",
    "    y_train = np.concatenate((y[idl], y[idm]))\n",
    "    return x_train, y_train\n",
    "\n",
    "#init_balanced_training_pool(x,y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = quarter_list[7]\n",
    "unlabeled_x2, test_x2,unlabeled_y2, test_y2, current_q2, current_y2 = quarter_list[8]\n",
    "np.concatenate((unlabeled_x,unlabeled_x2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_l = np.where(y == 0)[0]\n",
    "indices_m = np.where(y == 1)[0]\n",
    "idl = np.random.choice(indices_l, int(10/2))\n",
    "idm = np.random.choice(indices_m, 10 - int(10/2))\n",
    "\n",
    "indices_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[idl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((x[idl], x[idm]))\n",
    "y_train = np.concatenate((y[idl], y[idm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(20, size=5, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(df[(a['quarter']!= 1) | (a['year']!= 2011)].index)\n",
    "print(len(x))\n",
    "y = x.loc[:,'Malware'].to_numpy()\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(l, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def learning_forest(nb_query, x_pool, y_pool, x_train, y_train, show_query=True, show_it=True, threshold=0.95, show_balance=True):\n",
    "    \n",
    "    current_learner = ActiveLearner(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        query_strategy=uncertainty_sampling,\n",
    "        X_training=x_train, y_training=y_train\n",
    "    )\n",
    "    rec_list = []\n",
    "    prec_list = []\n",
    "    acc_list = []\n",
    "    F_list = []\n",
    "    it=[]\n",
    "    queried_all=[]\n",
    "    queried_mal = 0\n",
    "    min_wanted_query = 1\n",
    "    for nb_request in range(nb_query):\n",
    "        query_idx, query_inst = current_learner.query(x_pool)\n",
    "        current_learner.teach(X=x_pool[query_idx],\n",
    "                              y=y_pool[query_idx])\n",
    "        queried_all.append(y_pool[query_idx][0])\n",
    "        accuracy, recall, precision, F1 = calculate_acc(current_learner, x_pool, y_pool)\n",
    "        it.append(nb_request+1)\n",
    "        acc_list.append(accuracy)\n",
    "        rec_list.append(recall)\n",
    "        prec_list.append(precision)\n",
    "        F_list.append(F1)\n",
    "        if(accuracy>=threshold and min_wanted_query<2):\n",
    "            min_wanted_query = nb_request\n",
    "        if(show_it and (100*nb_request/nb_query)%10==0):\n",
    "            print(str(int(100*nb_request/nb_query))+'%')\n",
    "            \n",
    "    print('-----------------------------------------------------------------')\n",
    "    if(show_balance):\n",
    "        p=see_balance(y_pool, 'total pool')\n",
    "        t=see_balance(y_train, 'training pool')\n",
    "    queried_mal = len([i for i,x in enumerate(queried_all) if x == 1])\n",
    "    if(show_query):\n",
    "        print('malware query :'+str(queried_mal)+' ('+str(100*queried_mal/nb_query)+'%)')\n",
    "        print('legitimate query :'+str(nb_query-queried_mal)+' ('+str(100*(nb_query-queried_mal)/nb_query)+'%)')\n",
    "    print('final accuracy : '+str(accuracy)+' ; recall : '+str(recall)+' ; precision : '+str(precision))\n",
    "    print('minimum query before threshold ('+str(threshold*100)+'%) :'+str(min_wanted_query))\n",
    "    \n",
    "    return it, acc_list, rec_list, prec_list, F_list, queried_all\n",
    "  \n",
    "\n",
    "def calculate_acc(learner, data_x, data_y):\n",
    "  \n",
    "    pred = learner.predict(data_x)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(data_y, pred)\n",
    "    recall = metrics.recall_score(data_y, pred)\n",
    "    precision = metrics.precision_score(data_y, pred)\n",
    "    F1 = metrics.f1_score(data_y, pred)\n",
    "    \n",
    "    return accuracy, recall, precision, F1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#unlabeled_x, test_x,unlabeled_y, test_y  = init_quarter_pool(df, 1,2012)\n",
    "unlabeled_x, test_x,unlabeled_y, test_y, a, b = quarter_list_hyb[10]\n",
    "X_training, y_training = init_balanced_training_pool(unlabeled_x,unlabeled_y, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unlabeled_x, test_x,unlabeled_y, test_y  = init_quarter_pool(df, 1,2012)\n",
    "it, acc, rec, prec, F, query = learning_forest(300,unlabeled_x, unlabeled_y, X_training, y_training)\n",
    "\n",
    "\n",
    "plt.plot(it, acc, label = 'acc')\n",
    "plt.plot(it, F, label = 'F1')\n",
    "#plt.plot(it, rec, label = 'rec')\n",
    "#plt.plot(it, prec, label = 'prec')\n",
    "plt.legend()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_x, test_x,unlabeled_y, test_y  = init_quarter_pool(df, 1,2011)\n",
    "X_training, y_training = init_balanced_training_pool(unlabeled_x,unlabeled_y, 10)\n",
    "it, acc, rec, prec, F, query = learning_forest(300,test_pool_x, test_pool_y, X_training, y_training)\n",
    "\n",
    "\n",
    "plt.plot(it, acc, label = 'acc')\n",
    "plt.plot(it, F, label = 'F1')\n",
    "#plt.plot(it, rec, label = 'rec')\n",
    "#plt.plot(it, prec, label = 'prec')\n",
    "plt.legend()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overS_quarter_pool(data, quart, year):\n",
    "    x, test_x,y, test_y = init_quarter_pool(data, quart, year)\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "    x_over, y_over = oversample.fit_resample(x, y)\n",
    "    return x_over, test_x, y_over, test_y\n",
    "\n",
    "def underS_quarter_pool(data, quart, year):\n",
    "    x, test_x,y, test_y  = init_quarter_pool(data, quart, year)\n",
    "    undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "    x_over, y_over = undersample.fit_resample(x, y)\n",
    "    return x_over, test_x, y_over, test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_x, test_x,unlabeled_y, test_y  = init_quarter_pool(df, 1,2011)\n",
    "X_training, y_training = init_balanced_training_pool(unlabeled_x,unlabeled_y, 10)\n",
    "it, acc, rec, prec, F, query = learning_forest(300,unlabeled_x, unlabeled_y, X_training, y_training)\n",
    "\n",
    "\n",
    "plt.plot(it, acc, label = 'acc')\n",
    "plt.plot(it, F, label = 'F1')\n",
    "#plt.plot(it, rec, label = 'rec')\n",
    "#plt.plot(it, prec, label = 'prec')\n",
    "plt.legend()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_x, test_x,unlabeled_y, test_y  = init_quarter_pool(df, 1,2011)\n",
    "X_training, y_training = init_balanced_training_pool(unlabeled_x,unlabeled_y, 10)\n",
    "it, acc, rec, prec, F, query = learning_forest(300,unlabeled_x, unlabeled_y, X_training, y_training)\n",
    "\n",
    "\n",
    "plt.plot(it, acc, label = 'acc')\n",
    "plt.plot(it, F, label = 'F1')\n",
    "#plt.plot(it, rec, label = 'rec')\n",
    "#plt.plot(it, prec, label = 'prec')\n",
    "plt.legend()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SALT = 5\n",
    "rd_salt_range = 10\n",
    "\n",
    "def rd_sampling(classifier, X_pool):\n",
    "    n_samples = len(X_pool)\n",
    "    query_idx = np.random.choice(range(n_samples),replace=False)\n",
    "    return np.array([query_idx])\n",
    "\n",
    "def no_repeat_uncertainty(classifier, X_pool):\n",
    "    unc_list = classifier_uncertainty(classifier, X_pool)\n",
    "    query_idx=unc_list.argmax()\n",
    "    return np.array([query_idx])\n",
    "\n",
    "def salt_uncertainty(classifier, X_pool):\n",
    "    unc_list = classifier_uncertainty(classifier, X_pool)\n",
    "    for i in range(SALT):\n",
    "        if(len(unc_list)>1):\n",
    "            unc_list = np.delete(unc_list, unc_list.argmax())\n",
    "    query_idx=unc_list.argmax()\n",
    "    return np.array([query_idx])\n",
    "\n",
    "def random_salt_uncertainty(classifier, X_pool):\n",
    "    unc_list = classifier_uncertainty(classifier, X_pool)\n",
    "    salt = random.randint(0,rd_salt_range)\n",
    "    for i in range(salt):\n",
    "        if(len(unc_list)>1):\n",
    "            unc_list = np.delete(unc_list, unc_list.argmax())\n",
    "    query_idx=unc_list.argmax()\n",
    "    return np.array([query_idx])\n",
    "\n",
    "\n",
    "def learning_experiment_time(y1, q1, nb_q, data, mode, balancing, nb_query=200, show_query=False, show_it=True, threshold=0.95, show_balance=True, random = False):\n",
    "    #creation of the initial pools\n",
    "    \n",
    "    unlabeled_x, test_x,unlabeled_y, test_y  = init_quarter_pool(data, q1, y1)\n",
    "    X_training, y_training = init_balanced_training_pool(unlabeled_x,unlabeled_y, 10)  \n",
    "    \n",
    "    if (balancing == 'under'):\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        unlabeled_x, unlabeled_y = undersample.fit_resample(unlabeled_x, unlabeled_y)\n",
    "    elif(balancing == 'over'):\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "        unlabeled_x, unlabeled_y = oversample.fit_resample(unlabeled_x, unlabeled_y)       \n",
    "    else :\n",
    "        print('error : invalid balancing method')\n",
    "        return 0,0,0\n",
    "    \n",
    "    MAX_QUERY = len(unlabeled_y)\n",
    "    \n",
    "    #creation of the initial model\n",
    "    if(random):\n",
    "        current_learner = ActiveLearner(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        query_strategy=rd_sampling,\n",
    "        X_training=X_training, y_training=y_training\n",
    "        )\n",
    "    else :\n",
    "        current_learner = ActiveLearner(\n",
    "            estimator=RandomForestClassifier(),\n",
    "            query_strategy=uncertainty_sampling,\n",
    "            X_training=X_training, y_training=y_training\n",
    "        )\n",
    "    \n",
    "    acc_list = []\n",
    "    F_list = []\n",
    "    it=[]\n",
    "    required_querry=[]\n",
    "    \n",
    "    accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "    acc_list.append(accuracy)\n",
    "    F_list.append(F1)\n",
    "    \n",
    "    for nb_quarter in range(nb_q):\n",
    "        nb_request=0\n",
    "        condition = True\n",
    "        min_wanted_query = 0\n",
    "        \n",
    "        while(condition):\n",
    "            query_idx, query_inst = current_learner.query(unlabeled_x)\n",
    "            current_learner.teach(X=unlabeled_x[query_idx],\n",
    "                                  y=unlabeled_y[query_idx])\n",
    "            #queried_all.append(test_pool_y[query_idx][0])\n",
    "            nb_request +=1            \n",
    "            accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "            \n",
    "            acc_list.append(accuracy)\n",
    "            F_list.append(F1)\n",
    "            if(accuracy>=threshold and min_wanted_query<1):\n",
    "                min_wanted_query = nb_request\n",
    "            if(show_it and (100*nb_request/nb_query)%10==0):\n",
    "                print(str(int(100*nb_request/nb_query))+'%')\n",
    "                \n",
    "            if(mode == 'query'):\n",
    "                condition = nb_request < nb_query\n",
    "            elif(mode == 'acc'):\n",
    "                condition = (accuracy < threshold) & (nb_request < MAX_QUERY)\n",
    "            elif(mode == 'F1'):\n",
    "                condition = (F1 < threshold) & (nb_request < MAX_QUERY)            \n",
    "            else :\n",
    "                print('error : invalid mode')\n",
    "                return 0,0,0\n",
    "        \n",
    "        if(show_balance):\n",
    "            p=see_balance(unlabeled_y, 'total pool')\n",
    "            \n",
    "        if(min_wanted_query<1):\n",
    "            min_wanted_query = nb_request\n",
    "        print('final accuracy at q'+str(nb_quarter)+': '+str(accuracy))\n",
    "        q_ratio = 100*min_wanted_query/len(unlabeled_y)\n",
    "        required_querry.append((min_wanted_query,q_ratio))\n",
    "        print('minimum query before threshold ('+str(threshold*100)+'%) :'+str(min_wanted_query)+' ('+str(q_ratio)+'% of current pool size)')\n",
    "            \n",
    "        next_q = (q1+nb_quarter+1)%6\n",
    "        next_y = y1+(q1+nb_quarter+1)//6\n",
    "        if(next_q == 0):\n",
    "            next_q = 6\n",
    "        unlabeled_x, test_x,unlabeled_y, test_y = init_quarter_pool(data, next_q ,next_y)\n",
    "        MAX_QUERY = len(unlabeled_y)\n",
    "        print('transition accuracy :' + str(calculate_acc(current_learner, test_x, test_y)[0]))\n",
    "        print('-----------------------------------------------------------------')\n",
    "    \n",
    "    return acc_list, F_list, required_querry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_experiment3(y1, q1, nb_q, data, mode, balancing, nb_query=200, show_query=False, show_it=True, threshold=0.95, show_balance=True, sampling = uncertainty_sampling):\n",
    "    #creation of the initial pools\n",
    "    \n",
    "    next_it = q1 + 6*(y1-2009)\n",
    "    unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it-1]\n",
    "    current_pool_size = len(unlabeled_y)\n",
    "    MAX_QUERY = current_pool_size\n",
    "    \n",
    "    X_training, y_training = init_balanced_training_pool(unlabeled_x,unlabeled_y, 10)  \n",
    "    \n",
    "    if (balancing == 'under'):\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        unlabeled_x, unlabeled_y = undersample.fit_resample(unlabeled_x, unlabeled_y)\n",
    "    elif(balancing == 'over'):\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "        unlabeled_x, unlabeled_y = oversample.fit_resample(unlabeled_x, unlabeled_y)       \n",
    "    else :\n",
    "        print('error : invalid balancing method')\n",
    "        return 0,0,0\n",
    "    \n",
    "    \n",
    "    #creation of the initial model\n",
    "    \n",
    "    current_learner = ActiveLearner(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        query_strategy=sampling,\n",
    "        X_training=X_training, y_training=y_training\n",
    "    )\n",
    "    \n",
    "    acc_list = []\n",
    "    F_list = []\n",
    "    it=[]\n",
    "    required_querry=[]\n",
    "    learning_distrib = []\n",
    "    \n",
    "    accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "    acc_list.append(accuracy)\n",
    "    F_list.append(F1)\n",
    "    \n",
    "    for nb_quarter in range(nb_q):\n",
    "        nb_request=0\n",
    "        condition = True\n",
    "        min_wanted_query = 0\n",
    "        learning_rate = []\n",
    "        prev_learner = current_learner\n",
    "        \n",
    "        if(show_balance):\n",
    "            p=see_balance(unlabeled_y, 'total pool')\n",
    "        \n",
    "        while(condition):\n",
    "            query_idx, query_inst = current_learner.query(unlabeled_x)\n",
    "            current_learner.teach(X=unlabeled_x[query_idx],\n",
    "                                  y=unlabeled_y[query_idx])\n",
    "            #queried_all.append(test_pool_y[query_idx][0])\n",
    "            nb_request +=1\n",
    "                \n",
    "                \n",
    "            accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "            \n",
    "            acc_list.append(accuracy)\n",
    "            F_list.append(F1)\n",
    "            if(F1>=threshold and min_wanted_query<1):\n",
    "                min_wanted_query = nb_request\n",
    "            if(show_it and (100*nb_request/nb_query)%10==0):\n",
    "                print(str(int(100*nb_request/nb_query))+'%')\n",
    "                \n",
    "            if(mode == 'query'):\n",
    "                condition = nb_request < nb_query\n",
    "            elif(mode == 'acc'):\n",
    "                condition = (accuracy < threshold) & (nb_request < MAX_QUERY)\n",
    "            elif(mode == 'F1'):\n",
    "                condition = (F1 < threshold) & (nb_request < MAX_QUERY)    \n",
    "            else :\n",
    "                print('error : invalid mode')\n",
    "                return 0,0,0\n",
    "            \n",
    "            unlabeled_x = np.delete(unlabeled_x, query_idx, axis=0)\n",
    "            unlabeled_y = np.delete(unlabeled_y, query_idx)\n",
    "            \n",
    "            if(nb_request>1):\n",
    "                learning_rate.append(F_list[-1]-F_list[-2])\n",
    "            \n",
    "        if(min_wanted_query<1):\n",
    "            min_wanted_query = nb_request\n",
    "        print('final F1 at q'+str(nb_quarter)+': '+str(F1))\n",
    "        q_ratio = 100*min_wanted_query/current_pool_size\n",
    "        print('minimum query before threshold ('+str(threshold*100)+'%) :'+str(min_wanted_query)+' ('+str(q_ratio)+'% of current pool size)')\n",
    "        if(mode == 'query'):\n",
    "            min_wanted_query = nb_request\n",
    "            q_ratio = 100*min_wanted_query/current_pool_size\n",
    "        required_querry.append((min_wanted_query,q_ratio))\n",
    "        if(len(learning_rate)!=0):\n",
    "            print(' average learning rate : '+ str(np.average(learning_rate)))\n",
    "            learning_distrib.append(learning_rate)\n",
    "        else :\n",
    "            learning_distrib.append([0])\n",
    "            \n",
    "        if(F1<threshold):\n",
    "            current_learner = prev_learner\n",
    "            print('threshold not reached, model rollback')\n",
    "\n",
    "        next_it = current_q + 6*(current_y-2009)\n",
    "        unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it]\n",
    "        current_pool_size = len(unlabeled_y)\n",
    "        MAX_QUERY = current_pool_size\n",
    "        print('transition F1 :' + str(calculate_acc(current_learner, test_x, test_y)[3]))\n",
    "        print('-----------------------------------------------------------------')\n",
    "    \n",
    "    return acc_list, F_list, required_querry,learning_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,f,q=learning_experiment3(2009, 1, 5, quarter_list, 'thresh', 'over', threshold = 0.95, show_it = False)\n",
    "\n",
    "L= [e[1] for e in q]\n",
    "u = [i for i in range(len(a))]\n",
    "plt.plot(u, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_supervised(y1, q1, nb_q, data, threshold=0.95, show_balance=True):\n",
    "    \n",
    "    test_pool_x, test_pool_y = overS_quarter_pool(data, q1,y1)\n",
    "    \n",
    "    clf = RandomForestClassifier()\n",
    "    \n",
    "    acc_list = []\n",
    "    F_list = []\n",
    "    it=[]\n",
    "    required_querry=[]\n",
    "    \n",
    "    for nb_quarter in range(nb_q):\n",
    "        accuracy = 0\n",
    "        condition = True\n",
    "        min_wanted_query = 1\n",
    "        \n",
    "        clf.fit(test_pool_x, test_pool_y)\n",
    "        accuracy, recall, precision, F1 = calculate_acc(clf, test_pool_x, test_pool_y)\n",
    "        acc_list.append(accuracy)\n",
    "        F_list.append(F1)\n",
    "        \n",
    "        if(show_balance):\n",
    "            p=see_balance(test_pool_y, 'total pool')\n",
    "        print('accuracy at q'+str(nb_quarter)+': '+str(accuracy))\n",
    "                 \n",
    "        next_q = (q1+nb_quarter+1)%6\n",
    "        next_y = y1+(q1+nb_quarter+1)//6\n",
    "        if(next_q == 0):\n",
    "            next_q = 6\n",
    "        test_pool_x, test_pool_y = overS_quarter_pool(data, next_q ,next_y)\n",
    "        \n",
    "        accuracy, recall, precision, F1 = calculate_acc(clf, test_pool_x, test_pool_y)\n",
    "        acc_list.append(accuracy)\n",
    "        F_list.append(F1)\n",
    "        \n",
    "        print('-----------------------------------------------------------------')\n",
    "    \n",
    "    return acc_list, F_list\n",
    "\n",
    "def supervised(y1, q1, nb_q, data, threshold=0.95, show_balance=True):\n",
    "    \n",
    "    next_it = q1 + 6*(y1-2009)\n",
    "    unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it-1]\n",
    "    clf = RandomForestClassifier()\n",
    "    \n",
    "    acc_list = []\n",
    "    F_list = []\n",
    "    size=0\n",
    "    \n",
    "    for nb_quarter in range(nb_q):\n",
    "        accuracy = 0\n",
    "        \n",
    "        clf.fit(unlabeled_x, unlabeled_y)\n",
    "        accuracy, recall, precision, F1 = calculate_acc(clf, test_x, test_y)\n",
    "        acc_list.append(accuracy)\n",
    "        F_list.append(F1)\n",
    "        \n",
    "        if(show_balance):\n",
    "            p=see_balance(unlabeled_y, 'total pool')\n",
    "        print('accuracy at q'+str(nb_quarter)+': '+str(accuracy))\n",
    "                 \n",
    "        next_it+=1\n",
    "        t_unlabeled_x, t_test_x, t_unlabeled_y, t_test_y, current_q, current_y = data[next_it]\n",
    "        unlabeled_x = np.concatenate((unlabeled_x, t_unlabeled_x), axis = 0)\n",
    "        test_x = np.concatenate((test_x, t_test_x), axis = 0)\n",
    "        unlabeled_y = np.concatenate((unlabeled_y, t_unlabeled_y), axis = 0)\n",
    "        test_y = np.concatenate((test_y, t_test_y), axis = 0)\n",
    "        \n",
    "        print('-----------------------------------------------------------------')\n",
    "    size = unlabeled_y.size\n",
    "    print('Total pool size :'+str(size))\n",
    "    return acc_list, F_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_sup, F_sup = supervised(2011, 5, 46, quarter_list_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(acc_sup)/len(acc_sup))\n",
    "print(sum(F_sup)/len(F_sup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_list, F_list, MQ_list, learning_distrib = learning_experiment3(2011, 6, 44, quarter_list_hyb, 'F1', 'over', threshold = 0.98, show_it = False,)\n",
    "print('_________________________________________________________________')\n",
    "print('-----------------------------------------------------------------')\n",
    "#acc_rd, F_rd, MQ_rd, learning_distrib_rd = learning_experiment3(2011, 6, 44, quarter_list_hyb, 'F1', 'over', threshold = 0.98, show_it = False, sampling=rd_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l_un= [e[1] for e in MQ_list if(e[1]<100)]\n",
    "l_rd= [e[1] for e in MQ_rd if(e[1]<100)]\n",
    "ind = [i for i in range(len(l_un))]\n",
    "indrd = [i for i in range(len(l_rd))]\n",
    "\n",
    "L_un= [e[1] for e in MQ_list]\n",
    "L_rd= [e[1] for e in MQ_rd]\n",
    "indL = [i for i in range(len(L_un))]\n",
    "\n",
    "print(l_un)\n",
    "print(sum(l_un)/len(l_un))\n",
    "print('------------------')\n",
    "print(l_rd)\n",
    "print(sum(l_rd)/len(l_rd))\n",
    "\n",
    "plt.plot(ind, l_un, label = 'uncertainty')\n",
    "plt.plot(indrd, l_rd, label = 'random')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sum(l):\n",
    "    tot = 0\n",
    "    for e in l:\n",
    "        tot+=e\n",
    "    return tot\n",
    "print(L_un)\n",
    "print('average % of pool querried:')\n",
    "print(sum(L_un)/len(L_un))\n",
    "print('------------------')\n",
    "print(L_rd)\n",
    "print('average % of pool querried (rd):')\n",
    "print(sum(L_rd)/len(L_rd))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(indL, L_un, label = 'uncertainty')\n",
    "plt.plot(indL, L_rd, label = 'random')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MQ_ind1= []\n",
    "MQ_indrd= []\n",
    "summ=0\n",
    "for e in MQ_list:\n",
    "    summ+=e[0]\n",
    "    MQ_ind1.append(summ)\n",
    "print\n",
    "summ=0\n",
    "for e in MQ_rd:\n",
    "    summ+=e[0]\n",
    "    MQ_indrd.append(summ)\n",
    "temp1=[acc_list[i] for i in MQ_ind1]\n",
    "temp2=[acc_rd[i] for i in MQ_indrd]\n",
    "av1= sum(temp1)/len(temp1)\n",
    "av2= sum(temp2)/len(temp2)\n",
    "\n",
    "print('------------------')\n",
    "print('average acc (unc):')\n",
    "print(av1)\n",
    "print('------------------')\n",
    "print('average acc (rd):')\n",
    "print(av2)\n",
    "print('------------------')\n",
    "#print('average acc (sup):')\n",
    "#print(sum(F_sup)/len(F_sup))\n",
    "it1 = [i for i in range(len(temp1))]\n",
    "\n",
    "plt.plot(it1, temp1, label = 'unc')\n",
    "plt.plot(it1, temp2, label = 'rd')\n",
    "#plt.plot(it1, acc_sup, label = 'sup')\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MQ_indF1= []\n",
    "MQ_indFrd= []\n",
    "summ=0\n",
    "for e in MQ_list:\n",
    "    summ+=e[0]\n",
    "    MQ_indF1.append(summ)\n",
    "print\n",
    "summ=0\n",
    "for e in MQ_rd:\n",
    "    summ+=e[0]\n",
    "    MQ_indFrd.append(summ)\n",
    "F1=[F_list[i] for i in MQ_indF1]\n",
    "F2=[F_rd[i] for i in MQ_indFrd]\n",
    "av1= sum(F1)/len(F1)\n",
    "av2= sum(F2)/len(F2)\n",
    "\n",
    "print('F1 : unc')\n",
    "print(av1)\n",
    "print('F1 : rd')\n",
    "print(av2)\n",
    "\n",
    "it1 = [i for i in range(len(F1))]\n",
    "\n",
    "plt.plot(it1, F1, label = 'unc')\n",
    "plt.plot(it1, F2, label = 'rd')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_lab = sum([e[0] for e in MQ_list])\n",
    "full = int(sum([e[0]*100/e[1] for e in MQ_list]))\n",
    "print(full)\n",
    "print(full_lab)\n",
    "print(full_lab*100/full)\n",
    "\n",
    "full_lab = sum([e[0] for e in MQ_rd])\n",
    "full = int(sum([e[0]*100/e[1] for e in MQ_rd]))\n",
    "print(full)\n",
    "print(full_lab)\n",
    "print(full_lab*100/full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30\n",
    "#j = 42\n",
    "#ind_learning_distrib = [i for i in range(len(learning_distrib[j]))]\n",
    "#plt.plot(ind_learning_distrib, learning_distrib[j])\n",
    "\n",
    "def compare(l1, l2, j):\n",
    "    ind_learning_distrib = [i for i in range(len(l1[j]))]\n",
    "    cum=0\n",
    "    cumul = []\n",
    "    for e in l1[j]:\n",
    "        cum+=e\n",
    "        cumul.append(cum)\n",
    "\n",
    "    plt.plot(ind_learning_distrib, l1[j], label = 'learning rate')\n",
    "    plt.plot(ind_learning_distrib, cumul, label = 'cumul')\n",
    "    plt.legend()\n",
    "\n",
    "compare(learning_distrib, MQ_list,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_details(l, query, start):\n",
    "    Q = []\n",
    "    sum=0\n",
    "    for i in range(len(query)):\n",
    "        sum += query[i][0]\n",
    "        Q.append(sum)\n",
    "    print(Q[-1])\n",
    "    for i in range(start):\n",
    "        Q.pop(0)\n",
    "    \n",
    "    acc = l[Q[0]:]\n",
    "    mi = min(acc)\n",
    "    ma = max(acc)\n",
    "    plt.plot([i for i in range(len(acc))],acc)\n",
    "    print(Q)\n",
    "    for i in range(len(Q)):\n",
    "        plt.vlines(Q[i]-Q[0],mi, ma, colors='grey')\n",
    "print(len(F_list))\n",
    "see_details(F_list, MQ_list,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis(acc_list, F_list, MQ_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_list, F_list, MQ_list = learning_experiment_time(2012, 1, 6, df, 'query', 'under', nb_query = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_rd, F_rd, MQ_rd = learning_experiment_time(2012, 1, 6, df, 'query', 'under', nb_query = 150, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_sup, F_sup = over_supervised(2012, 1, 6, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = [i for i in range(len(acc_list))]\n",
    "sup=[]\n",
    "#for j in range(len(acc_sup)):\n",
    " #   for i in range(150):\n",
    "  #      sup.append(acc_sup[j])\n",
    "#plt.plot(it, sup, label = 'supervised')\n",
    "plt.plot(it, acc_list, label = 'uncertainty')\n",
    "plt.plot(it, acc_rd, label = 'random')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_rd, F_rd = learning_experiment3_under(2012, 1, 18, df, 'querry', nb_query = 150, random=True)\n",
    "acc_unc, F_unc = learning_experiment3_under(2012, 1, 18, df, 'querry', nb_query = 150)\n",
    "acc_ref, F_ref = under_supervised(2012, 1, 18, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = [i for i in range(len(acc_unc))]\n",
    "sup=[]\n",
    "for j in range(len(acc_ref)):\n",
    "    for i in range(150):\n",
    "        sup.append(acc_ref[j])\n",
    "plt.plot(it, sup, label = 'supervised')\n",
    "plt.plot(it, acc_unc, label = 'uncertainty')\n",
    "plt.plot(it, acc_rd, label = 'random')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_max = [i for i in range(len(acc_ref))]\n",
    "max_rd = []\n",
    "max_unc = []\n",
    "for j in range(1,19):\n",
    "    max_rd.append(acc_rd[j*150-1])\n",
    "    max_unc.append(acc_unc[j*150-1])\n",
    "plt.plot(it_max, acc_ref, label = 'supervised')\n",
    "plt.plot(it_max, max_unc, label = 'uncertainty')\n",
    "plt.plot(it_max, max_rd, label = 'random')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rd[149]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#\n",
    "# use thresh (still keep a query max)\n",
    "#\n",
    "# SEPARATE TESTING AND UNLABELED POOL\n",
    "#\n",
    "#only apply resampling to the first quarter\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rd, F_rd = learning_experiment3_over(2012, 1, 18, df, 'querry', nb_query = 150, random=True)\n",
    "acc_unc, F_unc = learning_experiment3_over(2012, 1, 18, df, 'querry', nb_query = 150)\n",
    "acc_ref, F_ref = over_supervised(2012, 1, 18, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = [i for i in range(len(acc_unc))]\n",
    "sup=[]\n",
    "for j in range(len(acc_ref)):\n",
    "    for i in range(150):\n",
    "        sup.append(acc_ref[j])\n",
    "plt.plot(it, sup, label = 'supervised')\n",
    "plt.plot(it, acc_unc, label = 'uncertainty')\n",
    "plt.plot(it, acc_rd, label = 'random')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_max = [i for i in range(len(acc_ref))]\n",
    "max_rd = []\n",
    "max_unc = []\n",
    "for j in range(1,19):\n",
    "    max_rd.append(acc_rd[j*150-1])\n",
    "    max_unc.append(acc_unc[j*150-1])\n",
    "plt.plot(it_max, acc_ref, label = 'supervised')\n",
    "plt.plot(it_max, max_unc, label = 'uncertainty')\n",
    "plt.plot(it_max, max_rd, label = 'random')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(acc_list, F_list, MQ_list):    \n",
    "    averages = []\n",
    "    standard = []\n",
    "    data_vector = [averages, standard]\n",
    "    MQ_ind1= []\n",
    "    summ=0\n",
    "    for e in MQ_list:\n",
    "        summ+=e[0]\n",
    "        MQ_ind1.append(summ)\n",
    "    for i in range(4):\n",
    "        MQ_ind1.pop()\n",
    "    acc=[acc_list[i] for i in MQ_ind1]\n",
    "    av1= sum(acc)/len(acc)\n",
    "    std1 = np.std(acc)\n",
    "    data_vector[0].append(av1)\n",
    "    data_vector[1].append(std1)\n",
    "\n",
    "    print('------------------')\n",
    "    print('average acc :')\n",
    "    print(av1)\n",
    "    print('std acc :')\n",
    "    print(std1)\n",
    "    print('------------------')\n",
    "\n",
    "    F1=[F_list[i] for i in MQ_ind1]\n",
    "    av2= sum(F1)/len(F1)\n",
    "    std2 = np.std(F1)\n",
    "    data_vector[0].append(av2)\n",
    "    data_vector[1].append(std2)\n",
    "\n",
    "    print('average F1 :')\n",
    "    print(av2)\n",
    "    print('std F1 :')\n",
    "    print(std2)\n",
    "    print('------------------')\n",
    "\n",
    "    nb_q = [e[0] for e in MQ_list]\n",
    "    full_lab = sum(nb_q)\n",
    "    full = int(sum([e[0]*100/e[1] for e in MQ_list]))\n",
    "    print(full)\n",
    "    print(full_lab)\n",
    "    print(full_lab*100/full)\n",
    "    data_vector[0].append(full_lab)\n",
    "    data_vector[1].append(np.std(nb_q))\n",
    "\n",
    "    print('------------------')\n",
    "    plt.plot([i for i in range(len(acc))], acc, label = 'acc')\n",
    "    plt.plot([i for i in range(len(F1))], F1, label = 'F1')\n",
    "    plt.legend()\n",
    "    return np.asarray(data_vector)\n",
    "\n",
    "def experiment( mode, nb, sampling = uncertainty_sampling, print_details = False, plot = False, nb_time_period = 44, thresh = 0.98):\n",
    "    data_vector=[]\n",
    "    for i in range(nb):\n",
    "        acc_list, F_list, MQ_list, dis = learning_experiment3(2011, 6, nb_time_period , quarter_list_hyb, 'F1', mode, threshold = thresh, show_it = False, sampling = sampling)\n",
    "        data_vector.append(analysis(acc_list, F_list, MQ_list))\n",
    "    av = [e[0] for e in data_vector]\n",
    "    std = [e[1] for e in data_vector]\n",
    "    \n",
    "    av_vector = np.average(av, axis=0, keepdims=True)\n",
    "    std_vector = np.std(std, axis=0, keepdims=True)\n",
    "    plt.legend()\n",
    "    print(av_vector)\n",
    "    print(std_vector)\n",
    "    return av_vector, std_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SALT = 1\n",
    "e1 = experiment('over', 5, sampling = salt_uncertainty, thresh = 0.98)\n",
    "SALT = 2\n",
    "e2 = experiment('over', 5, sampling = salt_uncertainty, thresh = 0.98)\n",
    "SALT = 3\n",
    "e3 = experiment('over', 5, sampling = salt_uncertainty, thresh = 0.98)\n",
    "SALT = 4\n",
    "e4 = experiment('over', 5, sampling = salt_uncertainty, thresh = 0.98)\n",
    "SALT = 5\n",
    "e5 = experiment('over', 5, sampling = salt_uncertainty, thresh = 0.98)\n",
    "print(e1)\n",
    "print(e2)\n",
    "print(e3)\n",
    "print(e4)\n",
    "print(e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e6 = experiment('over', 5, sampling = random_salt_uncertainty, thresh = 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "v1 = experiment('over', 5)\n",
    "v2 =experiment('over', 5, sampling = entropy_sampling)\n",
    "print(v1)\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with plateau detector\n",
    "def learning_experiment4(y1, q1, nb_q, data, mode, balancing, nb_query=200, show_query=False, show_it=True, threshold=0.95, show_balance=True, sampling = uncertainty_sampling):\n",
    "    #creation of the initial pools\n",
    "    \n",
    "    next_it = q1 + 6*(y1-2009)\n",
    "    unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it-1]\n",
    "    current_pool_size = len(unlabeled_y)\n",
    "    MAX_QUERY = current_pool_size\n",
    "    \n",
    "    X_training, y_training = init_balanced_training_pool(unlabeled_x,unlabeled_y, 10)  \n",
    "    \n",
    "    if (balancing == 'under'):\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        unlabeled_x, unlabeled_y = undersample.fit_resample(unlabeled_x, unlabeled_y)\n",
    "    elif(balancing == 'over'):\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "        unlabeled_x, unlabeled_y = oversample.fit_resample(unlabeled_x, unlabeled_y)       \n",
    "    else :\n",
    "        print('error : invalid balancing method')\n",
    "        return 0,0,0\n",
    "    \n",
    "    \n",
    "    #creation of the initial model\n",
    "    \n",
    "    current_learner = ActiveLearner(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        query_strategy=sampling,\n",
    "        X_training=X_training, y_training=y_training\n",
    "    )\n",
    "    \n",
    "    acc_list = []\n",
    "    F_list = []\n",
    "    it=[]\n",
    "    required_querry=[]\n",
    "    learning_distrib = []\n",
    "    \n",
    "    accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "    acc_list.append(accuracy)\n",
    "    F_list.append(F1)\n",
    "    \n",
    "    for nb_quarter in range(nb_q):\n",
    "        nb_request=0\n",
    "        condition = True\n",
    "        min_wanted_query = 0\n",
    "        learning_rate = []\n",
    "        prev_samples = []\n",
    "        first_rec = 0\n",
    "        sec_rec = 0\n",
    "        third_rec = 0\n",
    "\n",
    "        prev_learner = current_learner\n",
    "        \n",
    "        if(show_balance):\n",
    "            p=see_balance(unlabeled_y, 'total pool')\n",
    "        \n",
    "        while(condition):\n",
    "            query_idx, query_inst = current_learner.query(unlabeled_x)\n",
    "            current_learner.teach(X=unlabeled_x[query_idx],\n",
    "                                  y=unlabeled_y[query_idx])\n",
    "            #queried_all.append(test_pool_y[query_idx][0])\n",
    "            nb_request +=1\n",
    "                \n",
    "            new_sample = query_idx\n",
    "            if(nb_request == 1):\n",
    "                prev_samples.append([new_sample,1])\n",
    "            else : \n",
    "                temp = [e[0] for e in prev_samples]\n",
    "                if(query_idx in temp):\n",
    "                    i = temp.index(query_idx)\n",
    "                    prev_samples[i][1]+=1\n",
    "                    if(first_rec ==0):\n",
    "                        first_rec = nb_request\n",
    "                        print('first recurence at it '+str(first_rec))\n",
    "                    elif(sec_rec == 0):\n",
    "                        sec_rec = nb_request\n",
    "                        print('second recurence at it '+str(sec_rec))\n",
    "                    elif(third_rec == 0):\n",
    "                        third_rec = nb_request\n",
    "                        print('third recurence at it '+str(third_rec))\n",
    "                else :\n",
    "                    prev_samples.append([new_sample,1])\n",
    "                \n",
    "            accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "            \n",
    "            acc_list.append(accuracy)\n",
    "            F_list.append(F1)\n",
    "            if(F1>=threshold and min_wanted_query<1):\n",
    "                min_wanted_query = nb_request\n",
    "            if(show_it and (100*nb_request/nb_query)%10==0):\n",
    "                print(str(int(100*nb_request/nb_query))+'%')\n",
    "                \n",
    "            if(mode == 'query'):\n",
    "                condition = nb_request < nb_query\n",
    "            elif(mode == 'acc'):\n",
    "                condition = (accuracy < threshold) & (nb_request < MAX_QUERY)\n",
    "            elif(mode == 'F1'):\n",
    "                condition = (F1 < threshold) & (nb_request < MAX_QUERY)    \n",
    "            else :\n",
    "                print('error : invalid mode')\n",
    "                return 0,0,0\n",
    "            \n",
    "            #unlabeled_x = np.delete(unlabeled_x, query_idx, axis=0)\n",
    "            #unlabeled_y = np.delete(unlabeled_y, query_idx)\n",
    "            \n",
    "            if(nb_request>1):\n",
    "                learning_rate.append(F_list[-1]-F_list[-2])\n",
    "                if(nb_request >30 and p==0):\n",
    "                    if(np.average(learning_rate[-30:])==0):\n",
    "                        print('reach a plateau at :'+str(nb_request) + ' and F1 is :'+str(F1))\n",
    "                        p=1\n",
    "            else :\n",
    "                p=0\n",
    "            \n",
    "        if(min_wanted_query<1):\n",
    "            min_wanted_query = nb_request\n",
    "        print('final F1 at q'+str(nb_quarter)+': '+str(F1))\n",
    "\n",
    "        q_ratio = 100*min_wanted_query/current_pool_size\n",
    "        print('minimum query before threshold ('+str(threshold*100)+'%) :'+str(min_wanted_query)+' ('+str(q_ratio)+'% of current pool size)')\n",
    "        if(mode == 'query'):\n",
    "            min_wanted_query = nb_request\n",
    "            q_ratio = 100*min_wanted_query/current_pool_size\n",
    "        required_querry.append((min_wanted_query,q_ratio,[first_rec, sec_rec, third_rec]))\n",
    "        if(len(learning_rate)!=0):\n",
    "            print(' average learning rate : '+ str(np.average(learning_rate)))\n",
    "            learning_distrib.append(learning_rate)\n",
    "        else :\n",
    "            learning_distrib.append([0])\n",
    "            \n",
    "        if(F1<threshold):\n",
    "            current_learner = prev_learner\n",
    "            print('threshold not reached, model rollback')\n",
    "\n",
    "        next_it = current_q + 6*(current_y-2009)\n",
    "        unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it]\n",
    "        current_pool_size = len(unlabeled_y)\n",
    "        MAX_QUERY = current_pool_size\n",
    "        print('transition F1 :' + str(calculate_acc(current_learner, test_x, test_y)[3]))\n",
    "        print('-----------------------------------------------------------------')\n",
    "    \n",
    "    return acc_list, F_list, required_querry,learning_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#acc_list, F_list, MQ_list, learning_distrib = learning_experiment4(2011, 6, 44, quarter_list_hyb, 'F1', 'over', threshold = 0.98, show_it = False,)\n",
    "acc_list1, F_list1, MQ_list1, learning_distrib1 = learning_experiment4(2011, 6, 44, quarter_list_hyb, 'F1', 'over', threshold = 0.98, show_it = False, sampling = salt_uncertainty)\n",
    "#acc_list2, F_list2, MQ_list2, learning_distrib2 = learning_experiment4(2011, 6, 44, quarter_list_hyb, 'F1', 'over', threshold = 0.98, show_it = False, sampling = margin_sampling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 30, 31, 40 weird\n",
    "#28 clean\n",
    "def compare2(l1, l2, j):\n",
    "    ind_learning_distrib = [i for i in range(len(l1[j]))]\n",
    "    cum=0\n",
    "    cumul = []\n",
    "    for e in l1[j]:\n",
    "        cum+=e\n",
    "        cumul.append(cum)\n",
    "\n",
    "    plt.plot(ind_learning_distrib, l1[j], label = 'learning rate')\n",
    "    plt.plot(ind_learning_distrib, cumul, label = 'cumul')\n",
    "    plt.legend()\n",
    "    mi = min(cumul)\n",
    "    ma = max(cumul)\n",
    "    if(l2[j][2][0]!=0):\n",
    "        plt.vlines(l2[j][2][0],mi, ma, colors='green')\n",
    "    if(l2[j][2][1]!=0):\n",
    "        plt.vlines(l2[j][2][1],mi, ma, colors='blue')\n",
    "    if(l2[j][2][2]!=0):\n",
    "        plt.vlines(l2[j][2][2],mi, ma, colors='red')\n",
    "\n",
    "#print(np.average(learning_distrib[j][12:42]))\n",
    "#print(np.average(learning_distrib[j][12:42])==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(learning_distrib, MQ_list1, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_details2(l, query, start):\n",
    "    Q = []\n",
    "    sum=0\n",
    "    for i in range(len(query)):\n",
    "        sum += query[i][0]\n",
    "        Q.append(sum)\n",
    "    print(Q[-1])\n",
    "    for i in range(start):\n",
    "        Q.pop(0)\n",
    "    \n",
    "    acc = l[Q[0]:]\n",
    "    mi = min(acc)\n",
    "    ma = max(acc)\n",
    "    plt.plot([i for i in range(len(acc))],acc)\n",
    "    for i in range(len(Q)):\n",
    "        plt.vlines(Q[i]-Q[0],mi, ma, colors='red')\n",
    "        if(query[i+start][2]!=0):\n",
    "            plt.vlines(Q[i]-Q[0]+query[i+start][2],mi, ma, colors='blue')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_details2(F_list1, MQ_list1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_drift_dataset_report = Report(metrics=[\n",
    "\n",
    "    DatasetDriftMetric(),\n",
    "\n",
    "    DataDriftTable(),    \n",
    "\n",
    "])\n",
    "#ref = [e[0] for e in quarter_list_hyb]\n",
    "data_drift_dataset_report.run(reference_data=t0, current_data=t1)\n",
    "\n",
    "data_drift_dataset_report.show(mode='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = list(df_hyb.columns)\n",
    "header.pop(-1)\n",
    "header.pop(0)\n",
    "t0 = pd.DataFrame(quarter_list_hyb[24][0], columns = header)\n",
    "t1 = pd.DataFrame(quarter_list_hyb[25][0], columns = header)\n",
    "t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#18-> 35\n",
    "t2 = pd.DataFrame(quarter_list_hyb[34][0], columns = header)\n",
    "t3 = pd.DataFrame(quarter_list_hyb[35][0], columns = header)\n",
    "data_drift_dataset_report.run(reference_data=t2, current_data=t3)\n",
    "data_drift_dataset_report.show(mode='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = pd.DataFrame(quarter_list_hyb[35][0], columns = header)\n",
    "t3 = pd.DataFrame(quarter_list_hyb[36][0], columns = header)\n",
    "data_drift_dataset_report.run(reference_data=t2, current_data=t3)\n",
    "data_drift_dataset_report.show(mode='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t2 = pd.DataFrame(quarter_list_hyb[44][0], columns = header)\n",
    "t3 = pd.DataFrame(quarter_list_hyb[45][0], columns = header)\n",
    "data_drift_dataset_report.run(reference_data=t2, current_data=t3)\n",
    "data_drift_dataset_report.show(mode='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6 + 6*(2011-2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= data_drift_dataset_report.as_dict()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"metrics\"][0]['result']['share_of_drifted_columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TEST_PERIOD = 44\n",
    "q1 = 5\n",
    "y1 = 2011\n",
    "\n",
    "data_drift_dataset_report = Report(metrics=[\n",
    "\n",
    "    DatasetDriftMetric(),\n",
    "\n",
    "    DataDriftTable(),    \n",
    "\n",
    "])\n",
    "\n",
    "result = []\n",
    "for i in range(NB_TEST_PERIOD):\n",
    "    init = q1 + 6*(y1-2009)\n",
    "    prev = pd.DataFrame(quarter_list_hyb[init+i][0], columns = header)\n",
    "    current = pd.DataFrame(quarter_list_hyb[init+1+i][0], columns = header)\n",
    "    print(quarter_list_hyb[init+i][4])\n",
    "    print(quarter_list_hyb[init+i][5])\n",
    "    print('----------')\n",
    "\n",
    "    data_drift_dataset_report.run(reference_data=prev, current_data=current)\n",
    "    current_result = data_drift_dataset_report.as_dict()\n",
    "    print((current_result[\"metrics\"][0]['result']['share_of_drifted_columns']))\n",
    "    result.append(current_result[\"metrics\"][0]['result']['share_of_drifted_columns'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = [i for i in range(len(result))]\n",
    "plt.plot(ind, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with drift detect\n",
    "NB_TEST_PERIOD = 44\n",
    "q1 = 5\n",
    "y1 = 2011\n",
    "header = list(df_hyb.columns)\n",
    "header.pop(-1)\n",
    "header.pop(0)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "\n",
    "def learning_experiment5(y1, q1, nb_q, data, mode, balancing, nb_query=200, show_query=False, show_it=True, threshold=0.95, show_balance=True, sampling = uncertainty_sampling, drift_thresh = 0.5, nb_drift_rd_querry = 20):\n",
    "    #creation of the initial pools\n",
    "    \n",
    "    next_it = q1 + 6*(y1-2009)\n",
    "    unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it-1]\n",
    "    current_pool_size = len(unlabeled_y) \n",
    "    MAX_QUERY = current_pool_size\n",
    "    \n",
    "    X_training, y_training = init_balanced_training_pool(unlabeled_x,unlabeled_y, 10)  \n",
    "    \n",
    "    if (balancing == 'under'):\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        unlabeled_x, unlabeled_y = undersample.fit_resample(unlabeled_x, unlabeled_y)\n",
    "    elif(balancing == 'over'):\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "        unlabeled_x, unlabeled_y = oversample.fit_resample(unlabeled_x, unlabeled_y)       \n",
    "    else :\n",
    "        print('error : invalid balancing method')\n",
    "        return 0,0,0\n",
    "    \n",
    "    \n",
    "    #creation of the initial model\n",
    "    \n",
    "    current_learner = ActiveLearner(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        query_strategy=sampling,\n",
    "        X_training=X_training, y_training=y_training\n",
    "    )\n",
    "    \n",
    "    acc_list = []\n",
    "    F_list = []\n",
    "    it=[]\n",
    "    required_querry=[]\n",
    "    learning_distrib = []\n",
    "    \n",
    "    accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "    acc_list.append(accuracy)\n",
    "    F_list.append(F1)\n",
    "    \n",
    "    data_drift_dataset_report = Report(metrics=[\n",
    "    DatasetDriftMetric(),\n",
    "    DataDriftTable(),    ])\n",
    "    drift = [0]\n",
    "    \n",
    "    for nb_quarter in range(nb_q):\n",
    "        nb_request=0\n",
    "        condition = True\n",
    "        min_wanted_query = 0\n",
    "        learning_rate = []\n",
    "        prev_learner = current_learner\n",
    "        \n",
    "        if(show_balance):\n",
    "            p=see_balance(unlabeled_y, 'total pool')\n",
    "        \n",
    "        while(condition):\n",
    "            \n",
    "            if(drift[-1]<drift_thresh or nb_request > nb_drift_rd_querry):\n",
    "                query_idx, query_inst = current_learner.query(unlabeled_x)\n",
    "            else :\n",
    "                query_idx = rd_sampling(current_learner, unlabeled_x)\n",
    "            current_learner.teach(X=unlabeled_x[query_idx],\n",
    "                                  y=unlabeled_y[query_idx])\n",
    "            #queried_all.append(test_pool_y[query_idx][0])\n",
    "            nb_request +=1\n",
    "                \n",
    "                \n",
    "            accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "            \n",
    "            acc_list.append(accuracy)\n",
    "            F_list.append(F1)\n",
    "            if(F1>=threshold and min_wanted_query<1):\n",
    "                min_wanted_query = nb_request\n",
    "            if(show_it and (100*nb_request/nb_query)%10==0):\n",
    "                print(str(int(100*nb_request/nb_query))+'%')\n",
    "                \n",
    "            if(mode == 'query'):\n",
    "                condition = nb_request < nb_query\n",
    "            elif(mode == 'acc'):\n",
    "                condition = (accuracy < threshold) & (nb_request < MAX_QUERY)\n",
    "            elif(mode == 'F1'):\n",
    "                condition = (F1 < threshold) & (nb_request < MAX_QUERY)    \n",
    "            else :\n",
    "                print('error : invalid mode')\n",
    "                return 0,0,0\n",
    "            \n",
    "            unlabeled_x = np.delete(unlabeled_x, query_idx, axis=0)\n",
    "            unlabeled_y = np.delete(unlabeled_y, query_idx)\n",
    "            \n",
    "            if(nb_request>1):\n",
    "                learning_rate.append(F_list[-1]-F_list[-2])\n",
    "            \n",
    "        if(min_wanted_query<1):\n",
    "            min_wanted_query = nb_request\n",
    "        print('final F1 at q'+str(nb_quarter)+': '+str(F1))\n",
    "        q_ratio = 100*min_wanted_query/current_pool_size\n",
    "        print('minimum query before threshold ('+str(threshold*100)+'%) :'+str(min_wanted_query)+' ('+str(q_ratio)+'% of current pool size)')\n",
    "        if(mode == 'query'):\n",
    "            min_wanted_query = nb_request\n",
    "            q_ratio = 100*min_wanted_query/current_pool_size\n",
    "        required_querry.append((min_wanted_query,q_ratio))\n",
    "        if(len(learning_rate)!=0):\n",
    "            print(' average learning rate : '+ str(np.average(learning_rate)))\n",
    "            learning_distrib.append(learning_rate)\n",
    "        else :\n",
    "            learning_distrib.append([0])\n",
    "            \n",
    "        if(F1<threshold):\n",
    "            current_learner = prev_learner\n",
    "            print('threshold not reached, model rollback')\n",
    "\n",
    "        next_it = current_q + 6*(current_y-2009)\n",
    "        unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it]\n",
    "        current_pool_size = len(unlabeled_y)\n",
    "        \n",
    "        #drift calculation\n",
    "        prev_data = pd.DataFrame(data[next_it-1][0], columns = header)\n",
    "        cur_data = pd.DataFrame(data[next_it][0], columns = header)\n",
    "        data_drift_dataset_report.run(reference_data=prev_data, current_data=cur_data)\n",
    "        current_drift = data_drift_dataset_report.as_dict()\n",
    "        drift.append(current_drift[\"metrics\"][0]['result']['share_of_drifted_columns'])\n",
    "        \n",
    "        \n",
    "        MAX_QUERY = current_pool_size\n",
    "        print('transition F1 :' + str(calculate_acc(current_learner, test_x, test_y)[3]))\n",
    "        print('-----------------------------------------------------------------')\n",
    "    \n",
    "    return acc_list, F_list, required_querry,learning_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list, F_list, MQ_list, learning_distrib = learning_experiment5(2011, 6, 44, quarter_list_hyb, 'F1', 'over', threshold = 0.98, show_it = False, drift_thresh = 0.1, nb_drift_rd_querry = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis(acc_list, F_list, MQ_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list1, F_list1, MQ_list1, learning_distrib1 = learning_experiment5(2011, 6, 44, quarter_list_hyb, 'F1', 'over', threshold = 0.98, show_it = False, drift_thresh = 0.25, nb_drift_rd_querry = 20)\n",
    "analysis(acc_list1, F_list1, MQ_list1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
