{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modAL.models import ActiveLearner\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "from modAL.uncertainty import entropy_sampling\n",
    "from modAL.uncertainty import margin_sampling\n",
    "from modAL.uncertainty import classifier_uncertainty\n",
    "from modAL.batch import uncertainty_batch_sampling\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import DataDriftTable\n",
    "from evidently.metrics import DatasetDriftMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emu_l = pd.read_csv('../KronoDroid/emu_legitimate_v1.csv')\n",
    "print('emu legitimate done')\n",
    "df_emu_m = pd.read_csv('../KronoDroid/emu_malware_v1.csv')\n",
    "print('emu malware done')\n",
    "df_real_l = pd.read_csv('../KronoDroid/real_legitimate_v1.csv')\n",
    "print('real legitimate done')\n",
    "df_real_m = pd.read_csv('../KronoDroid/real_malware_v1.csv')\n",
    "print('real malware done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarter(str_date):\n",
    "    Quarter = 0\n",
    "    nb_err = 0\n",
    "    try:\n",
    "        date = datetime.strptime(str_date, '%m/%d/%Y')\n",
    "    except :\n",
    "        nb_err+=1\n",
    "        print('///////////////////////err////////////////////////')\n",
    "        print(str_date)\n",
    "        date = datetime.strptime('1/1/1980', '%m/%d/%Y')\n",
    "    if (date.month<3):\n",
    "        Quarter = 1\n",
    "    elif (date.month>2 and date.month<5):\n",
    "        Quarter = 2\n",
    "    elif (date.month>4 and date.month<7):\n",
    "        Quarter = 3\n",
    "    elif (date.month>6 and date.month<9):\n",
    "        Quarter = 4\n",
    "    elif (date.month>8 and date.month<11):\n",
    "        Quarter = 5\n",
    "    elif (date.month>10):\n",
    "        Quarter = 6\n",
    "    return (Quarter, date.year)\n",
    "\n",
    "def get_years_list(date_list):\n",
    "    dfy = pd.DataFrame(columns=['year'])\n",
    "    for i, date in enumerate(date_list):\n",
    "        dfy.loc[i] = get_quarter(date)[1]\n",
    "    return dfy['year']\n",
    "\n",
    "def get_quarter_list(date_list):\n",
    "    dfqy = pd.DataFrame(columns=['quarter','year'])\n",
    "    for i, date in enumerate(date_list):\n",
    "        dfqy.loc[i] = get_quarter(date)\n",
    "    return dfqy\n",
    "\n",
    "def see_balance(l, pool_name='pool'):\n",
    "    nb_malware =0\n",
    "    for i in l:\n",
    "        if(i.all()):\n",
    "            nb_malware+=1\n",
    "            \n",
    "    print(\"malware in \" + pool_name + \": \" + str(nb_malware) +' ('+str(100*nb_malware/len(l))+'%)')\n",
    "    print(\"normal in \" + pool_name + \": \" + str(len(l) - nb_malware) +' ('+str(100*(len(l)-nb_malware)/len(l))+'%)')\n",
    "    return nb_malware\n",
    "\n",
    "def init_quarter_pool(data, quart, year, ratio = 0.25):\n",
    "    y = data.loc[:,'Malware'].to_numpy()\n",
    "    \n",
    "    a = get_quarter_list(data['HighestModDate'])\n",
    "    \n",
    "    x = data.drop(data[(a['quarter']!= quart) | (a['year']!= year)].index)\n",
    "    y = x.loc[:,'Malware'].to_numpy()\n",
    "    x = x.drop(columns=['Malware', 'HighestModDate']).to_numpy()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=ratio)\n",
    "    while(not see_balance(y_test)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=ratio)\n",
    "    return [X_train, X_test, y_train, y_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define time period and separate features\n",
    "full = 66\n",
    "temp_m3 = df_real_m.copy()\n",
    "temp_l3 = df_real_l.copy()\n",
    "\n",
    "temp_m3.drop(temp_m3.columns[469:], axis=1, inplace=True)\n",
    "temp_m3.drop(temp_m3.columns[464:468], axis=1, inplace=True)\n",
    "temp_m3.drop(temp_m3.columns[0], axis=1, inplace=True)\n",
    "\n",
    "temp_l3.drop(temp_l3.columns[469:], axis=1, inplace=True)\n",
    "temp_l3.drop(temp_l3.columns[464:468], axis=1, inplace=True)\n",
    "temp_l3.drop(temp_l3.columns[0], axis=1, inplace=True)\n",
    "\n",
    "df_hyb = pd.concat([temp_l3, temp_m3], ignore_index=True)\n",
    "\n",
    "df_hyb.drop(df_hyb[df_hyb['HighestModDate'] == '1980-00-00'].index, inplace = True)\n",
    "df_hyb.reset_index(inplace=True, drop=True)\n",
    "df_hyb.drop(df_hyb[get_years_list(df_hyb['HighestModDate']) >2023].index, inplace = True)\n",
    "df_hyb.reset_index(inplace=True, drop=True)\n",
    "\n",
    "quarter_list_hyb = []\n",
    "quarter_list_sys = []\n",
    "quarter_list_perm = []\n",
    "q1 = 0\n",
    "y1 = 2009\n",
    "\n",
    "for nb_quarter in range (full):\n",
    "    next_q = (q1+nb_quarter)%6+1\n",
    "    next_y = y1+(q1+nb_quarter)//6\n",
    "    print(next_q)\n",
    "    print(next_y)\n",
    "    quarter_list_hyb.append(init_quarter_pool(df_hyb, next_q ,next_y))\n",
    "    quarter_list_hyb[-1].append(next_q)\n",
    "    quarter_list_hyb[-1].append(next_y)\n",
    "\n",
    "for nb_quarter in range (len(quarter_list_hyb)):\n",
    "    X,x,L,l,q,y = quarter_list_hyb[nb_quarter]\n",
    "    \n",
    "    curr_perm_train = np.delete(X, slice(289),1)\n",
    "    curr_perm_test = np.delete(x, slice(289),1)\n",
    "    quarter_list_sys.append([curr_perm_train,curr_perm_test,L,l,q,y])\n",
    "    \n",
    "    curr_sys_train = np.delete(X, slice(289,462),1)\n",
    "    curr_sys_test = np.delete(x, slice(289,462),1)\n",
    "    quarter_list_perm.append([curr_sys_train,curr_sys_test,L,l,q,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 30\n",
    "LOWER_THRESH_IT = 10\n",
    "\n",
    "def init_balanced_training_pool(x, y, size):\n",
    "    indices_l = np.where(y == 0)[0]\n",
    "    indices_m = np.where(y == 1)[0]\n",
    "    idl = np.random.choice(indices_l, int(size/2))\n",
    "    idm = np.random.choice(indices_m, size - int(size/2))\n",
    "    \n",
    "    x_train = np.concatenate((x[idl], x[idm]))\n",
    "    y_train = np.concatenate((y[idl], y[idm]))\n",
    "    return x_train, y_train\n",
    "  \n",
    "\n",
    "def calculate_acc(learner, data_x, data_y):\n",
    "  \n",
    "    pred = learner.predict(data_x)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(data_y, pred)\n",
    "    recall = metrics.recall_score(data_y, pred)\n",
    "    precision = metrics.precision_score(data_y, pred)\n",
    "    F1 = metrics.f1_score(data_y, pred)\n",
    "    \n",
    "    return accuracy, recall, precision, F1\n",
    "\n",
    "def threshold_const(iteration):\n",
    "    res = 0.97\n",
    "    return 1-res, 'quart'\n",
    "\n",
    "def threshold_asc(iteration):\n",
    "    res = 0.9 + 0.002*iteration\n",
    "    return 1-res, 'quart'\n",
    "    \n",
    "def threshold_desc(iteration):\n",
    "    res = 1 - 0.002*iteration\n",
    "    return 1-res, 'quart'\n",
    "\n",
    "def threshold_cheating_desc(iteration):\n",
    "    if (iteration<29 and iteration>27):\n",
    "        res = 0.99999\n",
    "    else :\n",
    "        res = 0.99 - 0.002*iteration\n",
    "    return 1-res, 'quart'\n",
    "\n",
    "def double_thresh(iteration):\n",
    "    t_pos = 1-0.97\n",
    "    t_neg = 1-0.9\n",
    "    return [t_pos, t_neg], 'quart'\n",
    "\n",
    "def double_thresh_inv(iteration):\n",
    "    t_pos = 1-0.97\n",
    "    t_neg = 1-0.99\n",
    "    return [t_pos, t_neg], 'quart'\n",
    "\n",
    "def double_thresh_desc(iteration):\n",
    "    t_pos = 1-0.99 + 0.002*iteration\n",
    "    t_neg = 1-0.9\n",
    "    return [t_pos, t_neg], 'quart'\n",
    "\n",
    "def double_thresh_asc(iteration):\n",
    "    t_pos = 1-0.9 - 0.002*iteration\n",
    "    t_neg = 1-0.9\n",
    "    return [t_pos, t_neg], 'quart'\n",
    "\n",
    "def double_thresh_desc_inv(iteration):\n",
    "    t_pos = 1-0.9\n",
    "    t_neg = 1-0.99 + 0.002*iteration\n",
    "    return [t_pos, t_neg], 'quart'\n",
    "\n",
    "def double_thresh_asc_inv(iteration):\n",
    "    t_pos = 1-0.9 \n",
    "    t_neg = 1-0.9 - 0.002*iteration\n",
    "    return [t_pos, t_neg], 'quart'\n",
    "\n",
    "def it_thresh_desc_squared(iteration):\n",
    "    res = (T_MIN-1)/(FURTHEST_THRESH_IT**2)*(iteration**2) +1\n",
    "    return 1-res, 'it'\n",
    "\n",
    "def it_thresh_desc_fourth(iteration):\n",
    "    res = (T_MIN-1)/(FURTHEST_THRESH_IT**4)*(iteration**4) +1\n",
    "    return 1-res, 'it'\n",
    "\n",
    "def it_thresh_desc_fourth_caped(iteration):\n",
    "    res = (T_MIN-1)/(FURTHEST_THRESH_IT**4)*(iteration**4) +1\n",
    "    return min(1-res, 1-T_MIN), 'it'\n",
    "\n",
    "def it_thresh_desc2(iteration):\n",
    "    res =  (1-T_MIN)/FURTHEST_THRESH_IT * iteration\n",
    "    return min(res, 1-T_MIN), 'it'\n",
    "\n",
    "def it_thresh_desc3(iteration):\n",
    "    if(iteration<LOWER_THRESH_IT):\n",
    "        res = 0\n",
    "    else :\n",
    "        res =  (1-T_MIN)/(FURTHEST_THRESH_IT+LOWER_THRESH_IT) * iteration\n",
    "    return min(res, 1-T_MIN), 'it'\n",
    "\n",
    "def it_thresh_desc4(iteration):\n",
    "    if(iteration<LOWER_THRESH_IT):\n",
    "        res = 0\n",
    "    else :\n",
    "        res =  (1-T_MIN)/(FURTHEST_THRESH_IT) * (iteration - LOWER_THRESH_IT)\n",
    "    return min(res, 1-T_MIN), 'it'\n",
    "\n",
    "def it_thresh_desc5(iteration):\n",
    "    if(iteration<LOWER_THRESH_IT):\n",
    "        res = 0\n",
    "    else :\n",
    "        res =  (1-T_MIN)\n",
    "    return res, 'it'\n",
    "\n",
    "def no_auto_label(iteration):\n",
    "    return 0, 'it'\n",
    "\n",
    "                                                                    \n",
    "def second_query(learner, unlabeled_pool, threshold_function, it):\n",
    "    threshold = threshold_function(it)[0]\n",
    "    predict = learner.predict(unlabeled_pool)\n",
    "    predict_unc = classifier_uncertainty(learner, unlabeled_pool)\n",
    "    if(type(threshold)==float or type(threshold)==int):\n",
    "        query_inst_list = [unlabeled_pool[i] for i,e in enumerate(predict_unc) if e<threshold]\n",
    "        query_idx_list = [i for i,e in enumerate(predict_unc) if e<threshold]\n",
    "    else :\n",
    "        query_inst_list1 = [unlabeled_pool[i] for i,e in enumerate(predict_unc) if (e<threshold[0] and predict[i]==1)]\n",
    "        query_idx_list1 = [i for i,e in enumerate(predict_unc) if (e<threshold[0] and predict[i]==1)]\n",
    "        query_inst_list2 = [unlabeled_pool[i] for i,e in enumerate(predict_unc) if (e<threshold[1] and predict[i]==0)]\n",
    "        query_idx_list2 = [i for i,e in enumerate(predict_unc) if (e<threshold[1] and predict[i]==0)]\n",
    "        query_inst_list = query_inst_list1 + query_inst_list2\n",
    "        query_idx_list = query_idx_list1 + query_idx_list2\n",
    "    return np.asarray(query_idx_list, dtype=int), np.asarray(query_inst_list), predict[query_idx_list]\n",
    "\n",
    "\n",
    "def sum(l):\n",
    "    tot = 0\n",
    "    for e in l:\n",
    "        tot+=e\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 20\n",
    "LOWER_THRESH_IT = 10\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "index = [i for i in range(30)]\n",
    "plt_test1 = [1-it_thresh_desc_fourth(index[i])[0] for i in range(len(index))]\n",
    "plt_test2 = [1-it_thresh_desc_squared(index[i])[0] for i in range(len(index))]\n",
    "plt_test3 = [1-it_thresh_desc_fourth_caped(index[i])[0] for i in range(len(index))]\n",
    "\n",
    "\n",
    "plt_lin1 = [1-it_thresh_desc2(index[i])[0] for i in range(len(index))]\n",
    "plt_lin2 = [1-it_thresh_desc3(index[i])[0] for i in range(len(index))]\n",
    "plt_lin3 = [1-it_thresh_desc4(index[i])[0] for i in range(len(index))]\n",
    "plt_lin4 = [1-it_thresh_desc5(index[i])[0] for i in range(len(index))]\n",
    "\n",
    "#print(len(index))\n",
    "#print(plt_test)\n",
    "ax1.plot(index,plt_test1, label = \"fourth\")\n",
    "ax1.plot(index,plt_test2, label = \"squared\")\n",
    "ax1.plot(index,plt_test3, label = \"fourth caped\")\n",
    "\n",
    "\n",
    "ax2.plot(index,plt_lin1, label = \"lin 1\")\n",
    "ax2.plot(index,plt_lin2, label = \"lin 2\")\n",
    "ax2.plot(index,plt_lin3, label = \"lin 3\")\n",
    "ax2.plot(index,plt_lin4, label = \"level\")\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax1.set_xlabel('iteration')\n",
    "ax2.set_xlabel('iteration')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rd_sampling(classifier, X_pool):\n",
    "    n_samples = len(X_pool)\n",
    "    query_idx = np.random.choice(range(n_samples),replace=False)\n",
    "    return np.array([query_idx])\n",
    "\n",
    "def learning_experiment3(y1, q1, nb_q, data, mode, balancing, threshold_function, threshold=0.98, nb_query=200, show_query=False, show_it=False, show_balance=True, random = False):\n",
    "    #creation of the initial pools\n",
    "    \n",
    "    next_it = q1 + 6*(y1-2009)\n",
    "    unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it-1]\n",
    "    \n",
    "    \n",
    "    X_training, y_training = init_balanced_training_pool(unlabeled_x,unlabeled_y, 10)  \n",
    "    \n",
    "    if (balancing == 'under'):\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        unlabeled_x, unlabeled_y = undersample.fit_resample(unlabeled_x, unlabeled_y)\n",
    "    elif(balancing == 'over'):\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "        unlabeled_x, unlabeled_y = oversample.fit_resample(unlabeled_x, unlabeled_y)       \n",
    "    else :\n",
    "        print('error : invalid balancing method')\n",
    "        return 0,0,0,0\n",
    "    current_pool_size = len(unlabeled_y)\n",
    "    MAX_QUERY = current_pool_size\n",
    "    \n",
    "    #creation of the initial model\n",
    "    if(random):\n",
    "        current_learner = ActiveLearner(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        query_strategy=rd_sampling,\n",
    "        X_training=X_training, y_training=y_training\n",
    "        )\n",
    "    else :\n",
    "        current_learner = ActiveLearner(\n",
    "            estimator=RandomForestClassifier(),\n",
    "            query_strategy=uncertainty_sampling,\n",
    "            X_training=X_training, y_training=y_training\n",
    "        )\n",
    "    \n",
    "    acc_list = []\n",
    "    F_list = []\n",
    "    it=[]\n",
    "    required_querry=[]\n",
    "    autolabeled = []\n",
    "    uncertainty_distrib = []\n",
    "    learning_distrib = []\n",
    "    \n",
    "    accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "    acc_list.append(accuracy)\n",
    "    F_list.append(F1)\n",
    "    \n",
    "    for nb_quarter in range(nb_q):\n",
    "        nb_request=0\n",
    "        nb_complement_label=0\n",
    "        nb_false_pos = 0\n",
    "        nb_false_neg = 0\n",
    "        condition = True\n",
    "        min_wanted_query = 0\n",
    "        prev_learner = current_learner\n",
    "        local_distrib =[]\n",
    "        \n",
    "        if(show_balance):\n",
    "            p=see_balance(unlabeled_y, 'total pool')\n",
    "            \n",
    "        learning_rate = []\n",
    "            \n",
    "        while(condition):\n",
    "            \n",
    "            predict = classifier_uncertainty(current_learner, unlabeled_x)\n",
    "            av_unc = np.average(predict)\n",
    "            local_distrib.append(predict)\n",
    "            if(nb_request==0):\n",
    "                print('average starting uncertainty :' + str(av_unc))\n",
    "            \n",
    "            ##second query/auto labeling process :\n",
    "            #also add element with good certainty\n",
    "            \n",
    "            #check if the dynamic threshold evolve with iteration or time quarter\n",
    "            if(nb_quarter!=0):\n",
    "                t_type = threshold_function(0)[1]\n",
    "                if(t_type == 'quart'):\n",
    "                    t_arg = nb_quarter\n",
    "                elif(t_type == 'it'):\n",
    "                    t_arg = nb_request\n",
    "                else :\n",
    "                    print('invalid threshold function type')\n",
    "                    return 0,0,0,0\n",
    "                \n",
    "                auto_label_thresh = threshold_function(t_arg)[0]\n",
    "                #check if we use different threshold for each label\n",
    "                if(type(auto_label_thresh)==list):\n",
    "                    auto_label_thresh_pos = auto_label_thresh[0]\n",
    "                    auto_label_thresh_neg = auto_label_thresh[1]\n",
    "                query_idx_list, query_inst_list, label_list = second_query(current_learner, unlabeled_x, threshold_function, t_arg)\n",
    "                nb_complement_label += len(query_idx_list)\n",
    "            else :\n",
    "                auto_label_thresh=0\n",
    "                query_idx_list = np.array([],dtype=int)\n",
    "                query_inst_list = np.array([],dtype=int)\n",
    "                label_list = np.array([],dtype=int)\n",
    "               \n",
    "            ##standart querry process :\n",
    "            query_idx, query_inst = current_learner.query(unlabeled_x)\n",
    "            nb_request +=1\n",
    "            \n",
    "            if(query_inst_list.size>0):\n",
    "                query_idx_list = np.concatenate((query_idx_list, query_idx), axis=0)\n",
    "                query_inst_list = np.concatenate((query_inst_list, query_inst), axis=0)\n",
    "                label_list=np.concatenate((label_list, unlabeled_y[query_idx]), axis=0)\n",
    "            else :\n",
    "                query_idx_list = query_idx\n",
    "                query_inst_list = query_inst\n",
    "                label_list = unlabeled_y[query_idx]\n",
    "            \n",
    "\n",
    "            current_learner.teach(X=query_inst_list,\n",
    "                                          y=label_list)\n",
    "            \n",
    "            for i,e in enumerate(label_list):\n",
    "                if(unlabeled_y[query_idx_list[i]] == 1 and e == 0):\n",
    "                    nb_false_pos+=1\n",
    "                elif(unlabeled_y[query_idx_list[i]] == 0 and e == 1):\n",
    "                    nb_false_neg+=1\n",
    "                \n",
    "            unlabeled_x = np.delete(unlabeled_x, query_idx_list, axis=0)\n",
    "            unlabeled_y = np.delete(unlabeled_y, query_idx_list)\n",
    "            \n",
    "            accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "            \n",
    "            \n",
    "            acc_list.append(accuracy)\n",
    "            F_list.append(F1)\n",
    "            \n",
    "            if(nb_request>1):\n",
    "                learning_rate.append(F_list[-1]-F_list[-2])\n",
    "            \n",
    "            if(accuracy>=threshold and min_wanted_query<1):\n",
    "                min_wanted_query = nb_request\n",
    "            if(show_it and (100*nb_request/nb_query)%10==0):\n",
    "                print(str(int(100*nb_request/nb_query))+'%')\n",
    "                \n",
    "            if(mode == 'query'):\n",
    "                condition = nb_request < nb_query\n",
    "            elif(mode == 'acc'):\n",
    "                condition = (accuracy < threshold) & (len(unlabeled_y) > 0)\n",
    "            elif(mode == 'F1'):\n",
    "                condition = (F1 < threshold) & (len(unlabeled_y) > 0)    \n",
    "            else :\n",
    "                print('error : invalid mode')\n",
    "                return 0,0,0\n",
    "        \n",
    "        uncertainty_distrib.append(local_distrib)\n",
    "        if(min_wanted_query<1):\n",
    "            min_wanted_query = nb_request\n",
    "        print('final F1 at q'+str(nb_quarter)+' ('+str(current_q)+'/'+str(current_y)+'): '+str(F1))\n",
    "        q_ratio = 100*min_wanted_query/current_pool_size\n",
    "        required_querry.append((min_wanted_query,q_ratio))\n",
    "        autolabeled.append((nb_complement_label, (nb_false_pos, nb_false_neg), av_unc))\n",
    "        print('minimum query before threshold ('+str(threshold*100)+'%) :'+str(min_wanted_query)+' ('+str(q_ratio)+'% of current pool size)')\n",
    "        if(len(learning_rate)!=0):\n",
    "            print(' average learning rate : '+ str(np.average(learning_rate)))\n",
    "            learning_distrib.append(np.average(learning_rate))\n",
    "        else :\n",
    "            learning_distrib.append(0)\n",
    "        if(type(auto_label_thresh)==list):\n",
    "            print('complement auto label :'+str(nb_complement_label) + ' with '+str(nb_false_pos)+' false pos (t = '+str(1-auto_label_thresh_pos)+') and '+ str(nb_false_neg)+' false neg (t = '+str(1-auto_label_thresh_neg)+')')\n",
    "        else :\n",
    "            print('complement auto label (t = '+str(1-auto_label_thresh)+') :'+str(nb_complement_label) + ' with '+str(nb_false_pos)+' false pos and '+ str(nb_false_neg)+' false neg' )\n",
    "\n",
    "        #if(F1<threshold):\n",
    "        #    current_learner = prev_learner\n",
    "            print('threshold not reached, model NOT rollback')\n",
    "\n",
    "        next_it = current_q + 6*(current_y-2009)\n",
    "        unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it]\n",
    "        current_pool_size = len(unlabeled_y)\n",
    "        MAX_QUERY = current_pool_size\n",
    "        print('transition F1 :' + str(calculate_acc(current_learner, test_x, test_y)[3]))\n",
    "        print('-----------------------------------------------------------------')\n",
    "    \n",
    "    return acc_list, F_list, required_querry, autolabeled, uncertainty_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FIG_HEIGHT = 6\n",
    "FIG_WIDTH = 12\n",
    "def analysis(acc_list, F_list, MQ_list, CL_list, plot):    \n",
    "    data_vector = []\n",
    "    MQ_ind1= []\n",
    "    summ=0\n",
    "    for e in MQ_list:\n",
    "        summ+=e[0]\n",
    "        MQ_ind1.append(summ)\n",
    "    acc=[acc_list[i] for i in MQ_ind1]\n",
    "    av1= sum(acc)/len(acc)\n",
    "    data_vector.append(av1*100)\n",
    "\n",
    "    print('------------------')\n",
    "    print('average acc :')\n",
    "    print(av1)\n",
    "    print('------------------')\n",
    "\n",
    "    F1=[F_list[i] for i in MQ_ind1]\n",
    "    av2= sum(F1)/len(F1)\n",
    "    data_vector.append(av2*100)\n",
    "\n",
    "    print('average F1 :')\n",
    "    print(av2)\n",
    "    print('------------------')\n",
    "\n",
    "    full_lab = sum([e[0] for e in MQ_list])\n",
    "    full = int(sum([e[0]*100/e[1] for e in MQ_list]))\n",
    "    print(full)\n",
    "    print(full_lab)\n",
    "    print(full_lab*100/full)\n",
    "    data_vector.append(full_lab)\n",
    "    print('------------------')\n",
    "\n",
    "    auto_lab = sum([e[0] for e in CL_list])\n",
    "    print('Autolabel:')\n",
    "    print(auto_lab)\n",
    "    data_vector.append(auto_lab)\n",
    "    print('------------------')\n",
    "\n",
    "    miss_lab_pos = sum([e[1][0] for e in CL_list])\n",
    "    miss_lab_neg = sum([e[1][1] for e in CL_list])\n",
    "    print('Misslabel:')\n",
    "    print('false positive :')\n",
    "    print(miss_lab_pos)\n",
    "    print('false negative :')\n",
    "    print(miss_lab_neg)\n",
    "    #data_vector.append(miss_lab)\n",
    "    print('------------------')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "    fig.set_size_inches(FIG_WIDTH, FIG_HEIGHT)\n",
    "    fig.suptitle('misslabeling and F1 evolution')\n",
    "    \n",
    "    plt_miss_pos = [e[1][0] for e in CL_list]\n",
    "    plt_miss_neg = [e[1][1] for e in CL_list]\n",
    "    plt_ind1 = [i for i in range(len(plt_miss_pos))]\n",
    "    plt_unc = [e[2] for e in CL_list]\n",
    "    \n",
    "    ax2.plot([i for i in range(len(F1))], F1)\n",
    "    ax1.plot(plt_ind1, plt_miss_pos, label = 'false pos')\n",
    "    ax1.plot(plt_ind1, plt_miss_neg, label = 'false neg')\n",
    "    ax1.legend()\n",
    "    return np.asarray(data_vector)\n",
    "\n",
    "def experiment(mode, nb, threshold_function, print_details = False, plot = False, nb_time_period = 44):\n",
    "    data_vector=[]\n",
    "    for i in range(nb):\n",
    "        acc_list, F_list, MQ_list, CL_list,dis = learning_experiment3(2011, 6, nb_time_period , quarter_list_hyb, 'F1', mode, threshold_function, threshold = 0.98, show_it = False)\n",
    "        data_vector.append(analysis(acc_list, F_list, MQ_list, CL_list, plot))\n",
    "    \n",
    "    av_vector = np.average(data_vector, axis=0, keepdims=True)\n",
    "    std_vector = np.std(data_vector, axis=0, keepdims=True)\n",
    "    plt.legend()\n",
    "\n",
    "    print(av_vector)\n",
    "    print(std_vector)\n",
    "    av_vector = [round(e,1) for e in av_vector[0]]\n",
    "    return av_vector, std_vector\n",
    "\n",
    "def experiment_drift(mode, nb, D_thresh, threshold_function = it_thresh_desc5, print_details = False, plot = False, nb_time_period = 44):\n",
    "    data_vector=[]\n",
    "    T_MIN = 0.9\n",
    "    FURTHEST_THRESH_IT = 30\n",
    "    LOWER_THRESH_IT = 30\n",
    "    for i in range(nb):\n",
    "        acc_list, F_list, MQ_list, CL_list,dis = learning_experiment4(2011, 6, nb_time_period , quarter_list_hyb, 'F1', mode, threshold_function, result, threshold = 0.98, drift_thresh = D_thresh)\n",
    "        data_vector.append(analysis(acc_list, F_list, MQ_list, CL_list, plot))\n",
    "    \n",
    "    av_vector = np.average(data_vector, axis=0, keepdims=True)\n",
    "    std_vector = np.std(data_vector, axis=0, keepdims=True)\n",
    "    plt.legend()\n",
    "    print(av_vector)\n",
    "    print(std_vector)\n",
    "    return av_vector, std_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_exp_under = []\n",
    "full_exp_over = []\n",
    "T_list = [0.1, 0.08, 0.05, 0.03, 0.02, 0.01]\n",
    "\n",
    "for t in T_list :\n",
    "    full_exp_under.append(experiment(t, 'under', 1, nb_time_period=44))\n",
    "for t in T_list :\n",
    "    full_exp_over.append(experiment(t, 'over', 1, nb_time_period=44))\n",
    "print(full_exp_over)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(0, 'under', 1, threshold_desc)\n",
    "experiment(0, 'under', 1, threshold_asc)\n",
    "experiment(0, 'over', 1, threshold_desc)\n",
    "experiment(0, 'over', 1, threshold_asc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#experiment(0, 'over', 4, threshold_cheating_desc)\n",
    "experiment(0, 'over', 1, threshold_cheating_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#acc_list, F_list, MQ_list, CL_list, distrib = learning_experiment3(2011, 6, 44 , quarter_list_hyb, 'F1', 'over', threshold_cheating_desc, threshold = 0.98)\n",
    "\n",
    "#analysis(acc_list, F_list, MQ_list, CL_list, True)\n",
    "acc_list1, F_list1, MQ_list1, CL_list1, distrib1 = learning_experiment3(2011, 6, 44 , quarter_list_sys, 'F1', 'over', threshold_const, threshold = 0.98)\n",
    "analysis(acc_list1, F_list1, MQ_list1, CL_list1, False)\n",
    "#acc_list2, F_list2, MQ_list2, CL_list2, distrib2 = learning_experiment3(2011, 6, 44 , quarter_list_perm, 'F1', 'over', threshold_cheating_desc, threshold = 0.98)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list2, F_list2, MQ_list2, CL_list2, distrib2 = learning_experiment3(2011, 6, 44 , quarter_list_sys, 'F1', 'over', double_thresh , threshold = 0.98)\n",
    "analysis(acc_list2, F_list2, MQ_list2, CL_list2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list3, F_list3, MQ_list3, CL_list3, distrib3 = learning_experiment3(2011, 6, 44 , quarter_list_sys, 'F1', 'over', double_thresh_inv , threshold = 0.98)\n",
    "analysis(acc_list3, F_list3, MQ_list3, CL_list3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_list, F_list, MQ_list, CL_list, distrib = learning_experiment3(2011, 6, 44 , quarter_list_sys, 'F1', 'over', double_thresh_desc , threshold = 0.98)\n",
    "analysis(acc_list, F_list, MQ_list, CL_list, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list, F_list, MQ_list, CL_list, distrib = learning_experiment3(2011, 6, 44 , quarter_list_sys, 'F1', 'over', double_thresh_asc, threshold = 0.98)\n",
    "analysis(acc_list, F_list, MQ_list, CL_list, False)\n",
    "\n",
    "#acc_list, F_list, MQ_list, CL_list, distrib = learning_experiment3(2011, 6, 44 , quarter_list_sys, 'F1', 'over', double_thresh, threshold = 0.98)\n",
    "#analysis(acc_list, F_list, MQ_list, CL_list, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_list, F_list, MQ_list, CL_list, distrib = learning_experiment3(2011, 6, 44 , quarter_list_sys, 'F1', 'over', double_thresh_asc_inv, threshold = 0.98)\n",
    "analysis(acc_list, F_list, MQ_list, CL_list, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for d in distrib1:\n",
    "    data.append(d[0])\n",
    "plt.boxplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_shape(distribution, prec = 10):\n",
    "    res = prec*[0]\n",
    "    for i in distribution:\n",
    "        ind = int(i*prec*2)-1\n",
    "        res[ind]+=1\n",
    "    abscisse = [i/(prec*2) for i in range(prec)]\n",
    "    return abscisse, res\n",
    "t = distrib[0][1]\n",
    "abscisse,res=display_shape(t)\n",
    "plt.hist(t)\n",
    "plt.plot(abscisse,res, label=str(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_details(l, query, start):\n",
    "    Q = []\n",
    "    sum=0\n",
    "    for i in range(len(query)):\n",
    "        sum += query[i][0]\n",
    "        Q.append(sum)\n",
    "    print(Q[-1])\n",
    "    for i in range(start):\n",
    "        Q.pop(0)\n",
    "    \n",
    "    acc = l[Q[0]:]\n",
    "    mi = min(acc)\n",
    "    ma = max(acc)\n",
    "    plt.plot([i for i in range(len(acc))],acc)\n",
    "    for i in range(len(Q)):\n",
    "        plt.vlines(Q[i]-Q[0],mi, ma, colors='red')\n",
    "    \n",
    "see_details(acc_list, MQ_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "see_details(F_list1, MQ_list1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for d in distrib:\n",
    "    data.append(d[0])\n",
    "plt.boxplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for d in distrib1:\n",
    "    data.append(d[0])\n",
    "plt.boxplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for d in distrib2:\n",
    "    data.append(d[0])\n",
    "plt.boxplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_list, F_list, MQ_list, CL_list, distrib = learning_experiment3(2011, 6, 44 , quarter_list_hyb, 'F1', 'over', it_thresh_desc_squared, threshold = 0.98)\n",
    "analysis(acc_list, F_list, MQ_list, CL_list, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_list, F_list, MQ_list, CL_list, distrib = learning_experiment3(2011, 6, 40 , quarter_list_hyb, 'F1', 'over', it_thresh_desc_fourth, threshold = 0.98)\n",
    "analysis(acc_list, F_list, MQ_list, CL_list, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_list, F_list, MQ_list, CL_list, distrib = learning_experiment3(2011, 6, 40 , quarter_list_hyb, 'F1', 'over', it_thresh_desc_fourth_caped, threshold = 0.98)\n",
    "analysis(acc_list, F_list, MQ_list, CL_list, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 30\n",
    "LOWER_THRESH_IT = 30\n",
    "acc_list, F_list, MQ_list, CL_list, distrib = learning_experiment3(2011, 6, 40 , quarter_list_hyb, 'F1', 'over', it_thresh_desc5, threshold = 0.98)\n",
    "analysis(acc_list, F_list, MQ_list, CL_list, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_list, F_list, MQ_list, CL_list, distrib = learning_experiment3(2011, 6, 40 , quarter_list_hyb, 'F1', 'over', no_auto_label, threshold = 0.98)\n",
    "analysis(acc_list, F_list, MQ_list, CL_list, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment('over', 10, it_thresh_desc4, nb_time_period = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e0 = experiment('over', 10, no_auto_label, nb_time_period = 40)\n",
    "e1 = experiment('over', 10, it_thresh_desc_fourth, nb_time_period = 40)\n",
    "e2 = experiment('over', 10, it_thresh_desc_squared, nb_time_period = 40)\n",
    "e3 = experiment('over', 10, it_thresh_desc2, nb_time_period = 40)\n",
    "e4 = experiment('over', 10, it_thresh_desc3, nb_time_period = 40)\n",
    "\n",
    "e5 = experiment('over', 10, threshold_desc, nb_time_period = 40)\n",
    "e6 = experiment('over', 10, threshold_asc, nb_time_period = 40)\n",
    "e7 = experiment('over', 10, it_thresh_desc_fourth_caped, nb_time_period = 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e0)\n",
    "print(e1)\n",
    "print(e2)\n",
    "print(e3)\n",
    "print(e4)\n",
    "print(e5)\n",
    "print(e6)\n",
    "print(e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizing the parameters for the level shape\n",
    "\n",
    "T_MIN = 0.9\n",
    "LOWER_THRESH_IT = 27\n",
    "e0 = experiment('over', 10, it_thresh_desc5, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "LOWER_THRESH_IT = 33\n",
    "e1 = experiment('over', 10, it_thresh_desc5, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "LOWER_THRESH_IT = 38\n",
    "e2 = experiment('over', 10, it_thresh_desc5, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "LOWER_THRESH_IT = 43\n",
    "e3 = experiment('over', 10, it_thresh_desc5, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 27\n",
    "e4 = experiment('over', 10, it_thresh_desc5, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 33\n",
    "e5 = experiment('over', 10, it_thresh_desc5, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 38\n",
    "e6 = experiment('over', 10, it_thresh_desc5, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 43\n",
    "e7 = experiment('over', 10, it_thresh_desc5, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 27\n",
    "e8 = experiment('over', 10, it_thresh_desc5, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 33\n",
    "e9 = experiment('over', 10, it_thresh_desc5, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 38\n",
    "e10 = experiment('over', 10, it_thresh_desc5, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 43\n",
    "e11 = experiment('over', 10, it_thresh_desc5, nb_time_period = 40)\n",
    "\n",
    "\n",
    "print(e0)\n",
    "print(e1)\n",
    "print(e2)\n",
    "print(e3)\n",
    "print(e4)\n",
    "print(e5)\n",
    "print(e6)\n",
    "print(e7)\n",
    "print(e8)\n",
    "print(e9)\n",
    "print(e10)\n",
    "print(e11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#optimizing the parameters for the lin3 shape\n",
    "\n",
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 30\n",
    "LOWER_THRESH_IT = 5\n",
    "e0 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 30\n",
    "LOWER_THRESH_IT = 10\n",
    "e1 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 30\n",
    "LOWER_THRESH_IT = 15\n",
    "e2 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 30\n",
    "LOWER_THRESH_IT = 20\n",
    "e3 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 5\n",
    "e4 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 10\n",
    "e5 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 15\n",
    "e6 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 20\n",
    "e7 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 5\n",
    "e8 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 10\n",
    "e9 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 15\n",
    "e10 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 20\n",
    "e11 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "\n",
    "\n",
    "print(e0)\n",
    "print(e1)\n",
    "print(e2)\n",
    "print(e3)\n",
    "print(e4)\n",
    "print(e5)\n",
    "print(e6)\n",
    "print(e7)\n",
    "print(e8)\n",
    "print(e9)\n",
    "print(e10)\n",
    "print(e11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 40\n",
    "LOWER_THRESH_IT = 5\n",
    "e0 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "LOWER_THRESH_IT = 10\n",
    "e1 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "LOWER_THRESH_IT = 15\n",
    "e2 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "LOWER_THRESH_IT = 20\n",
    "e3 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 5\n",
    "e4 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 10\n",
    "e5 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 15\n",
    "e6 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 20\n",
    "e7 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 5\n",
    "e8 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 10\n",
    "e9 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 15\n",
    "e10 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 20\n",
    "e11 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "\n",
    "\n",
    "print(e0)\n",
    "print(e1)\n",
    "print(e2)\n",
    "print(e3)\n",
    "print(e4)\n",
    "print(e5)\n",
    "print(e6)\n",
    "print(e7)\n",
    "print(e8)\n",
    "print(e9)\n",
    "print(e10)\n",
    "print(e11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 20\n",
    "LOWER_THRESH_IT = 5\n",
    "e0 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "LOWER_THRESH_IT = 10\n",
    "e1 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "LOWER_THRESH_IT = 15\n",
    "e2 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "LOWER_THRESH_IT = 20\n",
    "e3 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 5\n",
    "e4 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 10\n",
    "e5 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 15\n",
    "e6 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "LOWER_THRESH_IT = 20\n",
    "e7 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 5\n",
    "e8 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 10\n",
    "e9 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 15\n",
    "e10 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "LOWER_THRESH_IT = 20\n",
    "e11 = experiment('over', 3, it_thresh_desc4, nb_time_period = 40)\n",
    "\n",
    "\n",
    "print(e0)\n",
    "print(e1)\n",
    "print(e2)\n",
    "print(e3)\n",
    "print(e4)\n",
    "print(e5)\n",
    "print(e6)\n",
    "print(e7)\n",
    "print(e8)\n",
    "print(e9)\n",
    "print(e10)\n",
    "print(e11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizing the parameters for the polynomial shape\n",
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 20\n",
    "e0 = experiment('over', 5, it_thresh_desc_fourth_caped, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 30\n",
    "e1 = experiment('over', 5, it_thresh_desc_fourth_caped, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 40\n",
    "e2 = experiment('over', 5, it_thresh_desc_fourth_caped, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "FURTHEST_THRESH_IT = 20\n",
    "e3 = experiment('over', 5, it_thresh_desc_fourth_caped, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "FURTHEST_THRESH_IT = 30\n",
    "e4 = experiment('over', 5, it_thresh_desc_fourth_caped, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "FURTHEST_THRESH_IT = 40\n",
    "e5 = experiment('over', 5, it_thresh_desc_fourth_caped, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "FURTHEST_THRESH_IT = 20\n",
    "e6 = experiment('over', 5, it_thresh_desc_fourth_caped, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "FURTHEST_THRESH_IT = 30\n",
    "e7 = experiment('over', 5, it_thresh_desc_fourth_caped, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "FURTHEST_THRESH_IT = 40\n",
    "e8 = experiment('over', 5, it_thresh_desc_fourth_caped, nb_time_period = 40)\n",
    "\n",
    "print(e0)\n",
    "print(e1)\n",
    "print(e2)\n",
    "print(e3)\n",
    "print(e4)\n",
    "print(e5)\n",
    "print(e6)\n",
    "print(e7)\n",
    "print(e8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 20\n",
    "e0 = experiment('over', 5, it_thresh_desc_fourth, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 30\n",
    "e1 = experiment('over', 5, it_thresh_desc_fourth, nb_time_period = 40)\n",
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 40\n",
    "e2 = experiment('over', 5, it_thresh_desc_fourth, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "FURTHEST_THRESH_IT = 20\n",
    "e3 = experiment('over', 5, it_thresh_desc_fourth, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "FURTHEST_THRESH_IT = 30\n",
    "e4 = experiment('over', 5, it_thresh_desc_fourth, nb_time_period = 40)\n",
    "T_MIN = 0.85\n",
    "FURTHEST_THRESH_IT = 40\n",
    "e5 = experiment('over', 5, it_thresh_desc_fourth, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "FURTHEST_THRESH_IT = 20\n",
    "e6 = experiment('over', 5, it_thresh_desc_fourth, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "FURTHEST_THRESH_IT = 30\n",
    "e7 = experiment('over', 5, it_thresh_desc_fourth, nb_time_period = 40)\n",
    "T_MIN = 0.95\n",
    "FURTHEST_THRESH_IT = 40\n",
    "e8 = experiment('over', 5, it_thresh_desc_fourth, nb_time_period = 40)\n",
    "\n",
    "print(e0)\n",
    "print(e1)\n",
    "print(e2)\n",
    "print(e3)\n",
    "print(e4)\n",
    "print(e5)\n",
    "print(e6)\n",
    "print(e7)\n",
    "print(e8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict data drift\n",
    "NB_TEST_PERIOD = 44\n",
    "q1 = 5\n",
    "y1 = 2011\n",
    "\n",
    "header = list(df_hyb.columns)\n",
    "header.pop(-1)\n",
    "header.pop(0)\n",
    "\n",
    "data_drift_dataset_report = Report(metrics=[\n",
    "\n",
    "    DatasetDriftMetric(),\n",
    "\n",
    "    DataDriftTable(),    \n",
    "\n",
    "])\n",
    "\n",
    "result = []\n",
    "for i in range(NB_TEST_PERIOD):\n",
    "    init = q1 + 6*(y1-2009)\n",
    "    prev = pd.DataFrame(quarter_list_hyb[init+i][0], columns = header)\n",
    "    current = pd.DataFrame(quarter_list_hyb[init+1+i][0], columns = header)\n",
    "    print(quarter_list_hyb[init+i][4])\n",
    "    print(quarter_list_hyb[init+i][5])\n",
    "    print('----------')\n",
    "\n",
    "    data_drift_dataset_report.run(reference_data=prev, current_data=current)\n",
    "    current_result = data_drift_dataset_report.as_dict()\n",
    "    result.append(current_result[\"metrics\"][0]['result']['share_of_drifted_columns'])\n",
    "\n",
    "    print((current_result[\"metrics\"][0]['result']['share_of_drifted_columns']))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [0.13636363636363635, 0.19047619047619047, 0.24242424242424243, 0.13636363636363635, 0.08008658008658008, 0.11688311688311688, 0.16017316017316016, 0.1406926406926407, 0.18614718614718614, 0.19264069264069264, 0.08658008658008658, 0.09956709956709957, 0.11255411255411256, 0.08441558441558442, 0.15800865800865802, 0.1406926406926407, 0.12337662337662338, 0.22077922077922077, 0.15367965367965367, 0.1774891774891775, 0.11038961038961038, 0.19264069264069264, 0.19913419913419914, 0.21428571428571427, 0.24675324675324675, 0.22294372294372294, 0.29653679653679654, 0.30303030303030304, 0.16233766233766234, 0.11904761904761904, 0.12987012987012986, 0.14285714285714285, 0.3528138528138528, 0.20562770562770563, 0.33116883116883117, 0.2748917748917749, 0.27705627705627706, 0.11688311688311688, 0.30303030303030304, 0.23593073593073594, 0.08008658008658008, 0.05411255411255411, 0.0670995670995671, 0.07575757575757576]\n",
    "ind = [i for i in range(len(result))]\n",
    "plt.plot(ind, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TEST_PERIOD = 44\n",
    "q1 = 5\n",
    "y1 = 2011\n",
    "header = list(df_hyb.columns)\n",
    "header.pop(-1)\n",
    "header.pop(0)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "def learning_experiment4(y1, q1, nb_q, data, mode, balancing, threshold_function, drift_distrib, threshold=0.98, nb_query=200, show_query=False, show_it=False, show_balance=True, random = False, drift_thresh = 0.25):\n",
    "    #creation of the initial pools\n",
    "    \n",
    "    next_it = q1 + 6*(y1-2009)\n",
    "    unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it-1]\n",
    "    X_training, y_training = init_balanced_training_pool(unlabeled_x,unlabeled_y, 10)  \n",
    "    \n",
    "    if (balancing == 'under'):\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        unlabeled_x, unlabeled_y = undersample.fit_resample(unlabeled_x, unlabeled_y)\n",
    "    elif(balancing == 'over'):\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "        unlabeled_x, unlabeled_y = oversample.fit_resample(unlabeled_x, unlabeled_y)       \n",
    "    else :\n",
    "        print('error : invalid balancing method')\n",
    "        return 0,0,0,0\n",
    "    current_pool_size = len(unlabeled_y)\n",
    "    MAX_QUERY = current_pool_size\n",
    "    \n",
    "    #creation of the initial model\n",
    "    if(random):\n",
    "        current_learner = ActiveLearner(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        query_strategy=rd_sampling,\n",
    "        X_training=X_training, y_training=y_training\n",
    "        )\n",
    "    else :\n",
    "        current_learner = ActiveLearner(\n",
    "            estimator=RandomForestClassifier(),\n",
    "            query_strategy=uncertainty_sampling,\n",
    "            X_training=X_training, y_training=y_training\n",
    "        )\n",
    "    \n",
    "    acc_list = []\n",
    "    F_list = []\n",
    "    it=[]\n",
    "    required_querry=[]\n",
    "    autolabeled = []\n",
    "    uncertainty_distrib = []\n",
    "    learning_distrib = []\n",
    "    \n",
    "    accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "    acc_list.append(accuracy)\n",
    "    F_list.append(F1)\n",
    "    \n",
    "    #data_drift_dataset_report = Report(metrics=[\n",
    "    #DatasetDriftMetric(),\n",
    "    #DataDriftTable(),    ])\n",
    "    drift = drift_distrib\n",
    "    \n",
    "    \n",
    "    for nb_quarter in range(nb_q):\n",
    "        nb_request=0\n",
    "        nb_complement_label=0\n",
    "        nb_false_pos = 0\n",
    "        nb_false_neg = 0\n",
    "        condition = True\n",
    "        min_wanted_query = 0\n",
    "        prev_learner = current_learner\n",
    "        local_distrib =[]\n",
    "        \n",
    "        if(show_balance):\n",
    "            p=see_balance(unlabeled_y, 'total pool')\n",
    "            \n",
    "        learning_rate = []\n",
    "            \n",
    "        while(condition):\n",
    "            \n",
    "            predict = classifier_uncertainty(current_learner, unlabeled_x)\n",
    "            av_unc = np.average(predict)\n",
    "            local_distrib.append(predict)\n",
    "            if(nb_request==0):\n",
    "                print('average starting uncertainty :' + str(av_unc))\n",
    "            \n",
    "            ##second query/auto labeling process :\n",
    "            #also add element with good certainty\n",
    "            \n",
    "            #no auto labeling for first time period or when high drift detected\n",
    "            if(nb_quarter!=0 and drift[-1]<drift_thresh):\n",
    "                           \n",
    "                #check if the dynamic threshold evolve with iteration or time quarter\n",
    "                t_type = threshold_function(0)[1]\n",
    "                if(t_type == 'quart'):\n",
    "                    t_arg = nb_quarter\n",
    "                elif(t_type == 'it'):\n",
    "                    t_arg = nb_request\n",
    "                else :\n",
    "                    print('invalid threshold function type')\n",
    "                    return 0,0,0,0\n",
    "                \n",
    "                auto_label_thresh = threshold_function(t_arg)[0]\n",
    "\n",
    "                \n",
    "                #check if we use different threshold for each label\n",
    "                if(type(auto_label_thresh)==list):\n",
    "                    auto_label_thresh_pos = auto_label_thresh[0]\n",
    "                    auto_label_thresh_neg = auto_label_thresh[1]\n",
    "                query_idx_list, query_inst_list, label_list = second_query(current_learner, unlabeled_x, threshold_function, t_arg)\n",
    "                nb_complement_label += len(query_idx_list)\n",
    "            else :\n",
    "                auto_label_thresh=0\n",
    "                query_idx_list = np.array([],dtype=int)\n",
    "                query_inst_list = np.array([],dtype=int)\n",
    "                label_list = np.array([],dtype=int)\n",
    "                if(nb_quarter!=0 and nb_request ==0):\n",
    "                    print(\"drift detected\")\n",
    "                    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "               \n",
    "            ##standart querry process :\n",
    "            query_idx, query_inst = current_learner.query(unlabeled_x)\n",
    "            nb_request +=1\n",
    "            \n",
    "            if(query_inst_list.size>0):\n",
    "                query_idx_list = np.concatenate((query_idx_list, query_idx), axis=0)\n",
    "                query_inst_list = np.concatenate((query_inst_list, query_inst), axis=0)\n",
    "                label_list=np.concatenate((label_list, unlabeled_y[query_idx]), axis=0)\n",
    "            else :\n",
    "                query_idx_list = query_idx\n",
    "                query_inst_list = query_inst\n",
    "                label_list = unlabeled_y[query_idx]\n",
    "            \n",
    "\n",
    "            current_learner.teach(X=query_inst_list,\n",
    "                                          y=label_list)\n",
    "            \n",
    "            for i,e in enumerate(label_list):\n",
    "                if(unlabeled_y[query_idx_list[i]] == 1 and e == 0):\n",
    "                    nb_false_pos+=1\n",
    "                elif(unlabeled_y[query_idx_list[i]] == 0 and e == 1):\n",
    "                    nb_false_neg+=1\n",
    "                \n",
    "            unlabeled_x = np.delete(unlabeled_x, query_idx_list, axis=0)\n",
    "            unlabeled_y = np.delete(unlabeled_y, query_idx_list)\n",
    "            \n",
    "            accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "            \n",
    "            \n",
    "            acc_list.append(accuracy)\n",
    "            F_list.append(F1)\n",
    "            \n",
    "            if(nb_request>1):\n",
    "                learning_rate.append(F_list[-1]-F_list[-2])\n",
    "            \n",
    "            if(accuracy>=threshold and min_wanted_query<1):\n",
    "                min_wanted_query = nb_request\n",
    "            if(show_it and (100*nb_request/nb_query)%10==0):\n",
    "                print(str(int(100*nb_request/nb_query))+'%')\n",
    "                \n",
    "            if(mode == 'query'):\n",
    "                condition = nb_request < nb_query\n",
    "            elif(mode == 'acc'):\n",
    "                condition = (accuracy < threshold) & (len(unlabeled_y) > 0)\n",
    "            elif(mode == 'F1'):\n",
    "                condition = (F1 < threshold) & (len(unlabeled_y) > 0)    \n",
    "            else :\n",
    "                print('error : invalid mode')\n",
    "                return 0,0,0\n",
    "        \n",
    "        uncertainty_distrib.append(local_distrib)\n",
    "        if(min_wanted_query<1):\n",
    "            min_wanted_query = nb_request\n",
    "        print('final F1 at q'+str(nb_quarter)+' ('+str(current_q)+'/'+str(current_y)+'): '+str(F1))\n",
    "        q_ratio = 100*min_wanted_query/current_pool_size\n",
    "        required_querry.append((min_wanted_query,q_ratio))\n",
    "        autolabeled.append((nb_complement_label, (nb_false_pos, nb_false_neg), av_unc))\n",
    "        print('minimum query before threshold ('+str(threshold*100)+'%) :'+str(min_wanted_query)+' ('+str(q_ratio)+'% of current pool size)')\n",
    "        if(len(learning_rate)!=0):\n",
    "            print(' average learning rate : '+ str(np.average(learning_rate)))\n",
    "            learning_distrib.append(np.average(learning_rate))\n",
    "        else :\n",
    "            learning_distrib.append(0)\n",
    "        if(type(auto_label_thresh)==list):\n",
    "            print('complement auto label :'+str(nb_complement_label) + ' with '+str(nb_false_pos)+' false pos (t = '+str(1-auto_label_thresh_pos)+') and '+ str(nb_false_neg)+' false neg (t = '+str(1-auto_label_thresh_neg)+')')\n",
    "        else :\n",
    "            print('complement auto label (t = '+str(1-auto_label_thresh)+') :'+str(nb_complement_label) + ' with '+str(nb_false_pos)+' false pos and '+ str(nb_false_neg)+' false neg' )\n",
    "\n",
    "        if(F1<threshold):\n",
    "        #    current_learner = prev_learner\n",
    "            print('threshold not reached, model NOT rollback')\n",
    "        \n",
    "        next_it = current_q + 6*(current_y-2009)\n",
    "        unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it]\n",
    "        current_pool_size = len(unlabeled_y)\n",
    "        \n",
    "        #drift calculation\n",
    "        #prev_data = pd.DataFrame(data[next_it-1][0], columns = header)\n",
    "        #cur_data = pd.DataFrame(data[next_it][0], columns = header)\n",
    "        #data_drift_dataset_report.run(reference_data=prev_data, current_data=cur_data)\n",
    "        #current_drift = data_drift_dataset_report.as_dict()\n",
    "        #drift.append(current_drift[\"metrics\"][0]['result']['share_of_drifted_columns'])\n",
    "\n",
    "        MAX_QUERY = current_pool_size\n",
    "        print('transition F1 :' + str(calculate_acc(current_learner, test_x, test_y)[3]))\n",
    "        print('-----------------------------------------------------------------')\n",
    "        \n",
    "        \n",
    "    \n",
    "    return acc_list, F_list, required_querry, autolabeled, uncertainty_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t=0.25\n",
    "T_MIN = 0.9\n",
    "FURTHEST_THRESH_IT = 30\n",
    "LOWER_THRESH_IT = 30\n",
    "acc_list, F_list, MQ_list, CL_list, distrib = learning_experiment4(2011, 6, 40 , quarter_list_hyb, 'F1', 'over', it_thresh_desc5, result, threshold = 0.98)\n",
    "analysis(acc_list, F_list, MQ_list, CL_list, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list1, F_list1, MQ_list1, CL_list1, distrib1 = learning_experiment4(2011, 6, 40 , quarter_list_hyb, 'F1', 'over', it_thresh_desc2,result, threshold = 0.98, drift_thresh = 0.2)\n",
    "analysis(acc_list1, F_list1, MQ_list1, CL_list1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it t min = 20\n",
    "#t min = 0.9\n",
    "\n",
    "acc_list2, F_list2, MQ_list2, CL_list2, distrib2 = learning_experiment4(2011, 6, 44 , quarter_list_hyb, 'F1', 'over', it_thresh_desc2, threshold = 0.98, drift_thresh = 0.3)\n",
    "analysis(acc_list2, F_list2, MQ_list2, CL_list2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(l1, l2, query1, query2, T, lab1 = 'l1', lab2 = 'l2'):\n",
    "    start1=1\n",
    "    start2=1\n",
    "    for i in range(T):\n",
    "        start1 += query1[i][0]\n",
    "        start2 += query2[i][0]\n",
    "    \n",
    "    ind1 = [i for i in range(query1[T][0])]\n",
    "    ind2 = [i for i in range(query2[T][0])]\n",
    "\n",
    "    plt.plot(ind1, l1[start1:start1+query1[T][0]], label = lab1)\n",
    "    plt.plot(ind2, l2[start2:start2+query2[T][0]], label = lab2)\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(F_list1, F_list2, MQ_list1, MQ_list2,18, lab1 = 0.2, lab2 = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "acc_list3, F_list3, MQ_list3, CL_list3, distrib3 = learning_experiment4(2011, 6, 44 , quarter_list_hyb, 'F1', 'over', it_thresh_desc2, threshold = 0.98, drift_thresh = 0.3)\n",
    "analysis(acc_list3, F_list3, MQ_list3, CL_list3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "acc_list4, F_list4, MQ_list4, CL_list4, distrib4 = learning_experiment4(2011, 6, 44 , quarter_list_hyb, 'F1', 'over', it_thresh_desc2, threshold = 0.98, drift_thresh = 0.3)\n",
    "analysis(acc_list4, F_list4, MQ_list4, CL_list4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "acc_list4, F_list4, MQ_list4, CL_list4, distrib4 = learning_experiment4(2011, 6, 44 , quarter_list_hyb, 'F1', 'over', it_thresh_desc2, threshold = 0.98, drift_thresh = 0.3)\n",
    "analysis(acc_list4, F_list4, MQ_list4, CL_list4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "acc_list5, F_list5, MQ_list5, CL_list5, distrib5 = learning_experiment4(2011, 6, 44 , quarter_list_hyb, 'F1', 'over', it_thresh_desc2, threshold = 0.98, drift_thresh = 0.25)\n",
    "analysis(acc_list5, F_list5, MQ_list5, CL_list5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_list6, F_list6, MQ_list6, CL_list6, distrib6 = learning_experiment4(2011, 6, 44 , quarter_list_hyb, 'F1', 'over', it_thresh_desc3, threshold = 0.98, drift_thresh = 0.5)\n",
    "analysis(acc_list6, F_list6, MQ_list6, CL_list6, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_list1, F_list1, MQ_list1, CL_list1, distrib1 = learning_experiment4(2011, 6, 44 , quarter_list_hyb, 'F1', 'over', it_thresh_desc3, threshold = 0.98, drift_thresh = 0.2)\n",
    "analysis(acc_list1, F_list1, MQ_list1, CL_list1, False)\n",
    "acc_list2, F_list2, MQ_list2, CL_list2, distrib2 = learning_experiment4(2011, 6, 44 , quarter_list_hyb, 'F1', 'over', it_thresh_desc3, threshold = 0.98, drift_thresh = 0.25)\n",
    "analysis(acc_list2, F_list2, MQ_list2, CL_list2, False)\n",
    "acc_list3, F_list3, MQ_list3, CL_list3, distrib3 = learning_experiment4(2011, 6, 44 , quarter_list_hyb, 'F1', 'over', it_thresh_desc3, threshold = 0.98, drift_thresh = 0.3)\n",
    "analysis(acc_list3, F_list3, MQ_list3, CL_list3, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis(acc_list6, F_list6, MQ_list6, CL_list6, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis(acc_list5, F_list5, MQ_list5, CL_list5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "ed0 = experiment_drift('over', 4, 0.2, nb_time_period = 40)\n",
    "ed1 = experiment_drift('over', 4, 0.25, nb_time_period = 40)\n",
    "ed2 = experiment_drift('over', 4, 0.3, nb_time_period = 40)\n",
    "ed3 = experiment_drift('over', 4, 0.5, nb_time_period = 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ed0)\n",
    "print(ed1)\n",
    "print(ed2)\n",
    "print(ed3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#improved the code, use different t strat\n",
    "\n",
    "ed0 = experiment_drift('over', 10, 0.2, nb_time_period = 40)\n",
    "ed1 = experiment_drift('over', 10, 0.25, nb_time_period = 40)\n",
    "ed2 = experiment_drift('over', 10, 0.3, nb_time_period = 40)\n",
    "ed3 = experiment_drift('over', 10, 0.5, nb_time_period = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ed0)\n",
    "print(ed1)\n",
    "print(ed2)\n",
    "print(ed3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TEST_PERIOD = 44\n",
    "q1 = 5\n",
    "y1 = 2011\n",
    "\n",
    "#uncertainty evolution measurement\n",
    "\n",
    "def learning_experiment5(y1, q1, nb_q, data, mode, balancing, threshold=0.98, nb_query=200, show_query=False, show_it=False, show_balance=True, random = False):\n",
    "    #creation of the initial pools\n",
    "    \n",
    "    next_it = q1 + 6*(y1-2009)\n",
    "    unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it-1]\n",
    "    X_training, y_training = init_balanced_training_pool(unlabeled_x,unlabeled_y, 10)  \n",
    "    \n",
    "    if (balancing == 'under'):\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        unlabeled_x, unlabeled_y = undersample.fit_resample(unlabeled_x, unlabeled_y)\n",
    "    elif(balancing == 'over'):\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "        unlabeled_x, unlabeled_y = oversample.fit_resample(unlabeled_x, unlabeled_y)       \n",
    "    else :\n",
    "        print('error : invalid balancing method')\n",
    "        return 0,0,0,0\n",
    "    current_pool_size = len(unlabeled_y)\n",
    "    MAX_QUERY = current_pool_size\n",
    "    \n",
    "    \n",
    "    #creation of the initial model\n",
    "    if(random):\n",
    "        current_learner = ActiveLearner(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        query_strategy=rd_sampling,\n",
    "        X_training=X_training, y_training=y_training\n",
    "        )\n",
    "    else :\n",
    "        current_learner = ActiveLearner(\n",
    "            estimator=RandomForestClassifier(),\n",
    "            query_strategy=uncertainty_sampling,\n",
    "            X_training=X_training, y_training=y_training\n",
    "        )\n",
    "    #reference supervised model we want to measure the evolution of\n",
    "    ref_supervised_learner = RandomForestClassifier()\n",
    "    \n",
    "    acc_list = []\n",
    "    F_list = []\n",
    "    it=[]\n",
    "    required_querry=[]\n",
    "    autolabeled = []\n",
    "    uncertainty_distrib = []\n",
    "    learning_distrib = []\n",
    "    \n",
    "    acc_ref =[]\n",
    "    F_ref = []\n",
    "    \n",
    "    ref_supervised_learner.fit(unlabeled_x, unlabeled_y)\n",
    "    acc, recall, precision, F = calculate_acc(ref_supervised_learner, test_x, test_y)\n",
    "    acc_ref.append(acc)\n",
    "    F_ref.append(F)\n",
    "    accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "    acc_list.append(accuracy)\n",
    "    F_list.append(F1)\n",
    "    \n",
    "    unc_evolution = []\n",
    "    measure_acquired = False\n",
    "    \n",
    "    for nb_quarter in range(nb_q):\n",
    "        nb_request=0\n",
    "        nb_complement_label=0\n",
    "        nb_false_pos = 0\n",
    "        nb_false_neg = 0\n",
    "        condition = True\n",
    "        min_wanted_query = 0\n",
    "        prev_learner = current_learner\n",
    "        local_distrib =[]\n",
    "        \n",
    "        if(show_balance):\n",
    "            p=see_balance(unlabeled_y, 'total pool')\n",
    "            \n",
    "        learning_rate = []\n",
    "            \n",
    "        while(condition):\n",
    "            \n",
    "            predict = classifier_uncertainty(current_learner, unlabeled_x)\n",
    "            av_unc = np.average(predict)\n",
    "            local_distrib.append(predict)\n",
    "            if(nb_request==0):\n",
    "                print('average starting uncertainty :' + str(av_unc))\n",
    "            \n",
    "            ##second query/auto labeling process :\n",
    "            #only does auto lbeling once, then keep track of th auto labelled element to see how their uncertainty evolve\n",
    "            \n",
    "            \n",
    "            if(not nb_quarter==0 and measure_acquired==False):\n",
    "                query_idx_list, query_inst_list, label_list = second_query(current_learner, unlabeled_x, threshold_const, 1)\n",
    "                measurement = [query_idx_list, query_inst_list, label_list]\n",
    "                measure_acquired = True\n",
    "            \n",
    "            query_idx_list = np.array([],dtype=int)\n",
    "            query_inst_list = np.array([],dtype=int)\n",
    "            label_list = np.array([],dtype=int)\n",
    "\n",
    "\n",
    "            \n",
    "                \n",
    "            ##standart querry process :\n",
    "            query_idx, query_inst = current_learner.query(unlabeled_x)\n",
    "            nb_request +=1\n",
    "            \n",
    "            if(query_inst_list.size>0):\n",
    "                query_idx_list = np.concatenate((query_idx_list, query_idx), axis=0)\n",
    "                query_inst_list = np.concatenate((query_inst_list, query_inst), axis=0)\n",
    "                label_list=np.concatenate((label_list, unlabeled_y[query_idx]), axis=0)\n",
    "            else :\n",
    "                query_idx_list = query_idx\n",
    "                query_inst_list = query_inst\n",
    "                label_list = unlabeled_y[query_idx]\n",
    "            \n",
    "\n",
    "            current_learner.teach(X=query_inst_list,\n",
    "                                          y=label_list)\n",
    "            \n",
    "            for i,e in enumerate(label_list):\n",
    "                if(unlabeled_y[query_idx_list[i]] == 1 and e == 0):\n",
    "                    nb_false_pos+=1\n",
    "                elif(unlabeled_y[query_idx_list[i]] == 0 and e == 1):\n",
    "                    nb_false_neg+=1\n",
    "                \n",
    "            unlabeled_x = np.delete(unlabeled_x, query_idx_list, axis=0)\n",
    "            unlabeled_y = np.delete(unlabeled_y, query_idx_list)\n",
    "            \n",
    "            acc, recall, precision, F = calculate_acc(ref_supervised_learner, test_x, test_y)\n",
    "            acc_ref.append(acc)\n",
    "            F_ref.append(F)\n",
    "            \n",
    "            accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "            \n",
    "            \n",
    "            acc_list.append(accuracy)\n",
    "            F_list.append(F1)\n",
    "            \n",
    "            if(nb_request>1):\n",
    "                learning_rate.append(F_list[-1]-F_list[-2])\n",
    "            \n",
    "            if(accuracy>=threshold and min_wanted_query<1):\n",
    "                min_wanted_query = nb_request\n",
    "            if(show_it and (100*nb_request/nb_query)%10==0):\n",
    "                print(str(int(100*nb_request/nb_query))+'%')\n",
    "                \n",
    "            if(mode == 'query'):\n",
    "                condition = nb_request < nb_query\n",
    "            elif(mode == 'acc'):\n",
    "                condition = (accuracy < threshold) & (len(unlabeled_y) > 0)\n",
    "            elif(mode == 'F1'):\n",
    "                condition = (F1 < threshold) & (len(unlabeled_y) > 0)    \n",
    "            else :\n",
    "                print('error : invalid mode')\n",
    "                return 0,0,0,0,0\n",
    "\n",
    "        uncertainty_distrib.append(local_distrib)\n",
    "        if(min_wanted_query<1):\n",
    "            min_wanted_query = nb_request\n",
    "        print('final F1 at q'+str(nb_quarter)+' ('+str(current_q)+'/'+str(current_y)+'): '+str(F1))\n",
    "        q_ratio = 100*min_wanted_query/current_pool_size\n",
    "        required_querry.append((min_wanted_query,q_ratio))\n",
    "        autolabeled.append((nb_complement_label, (nb_false_pos, nb_false_neg), av_unc))\n",
    "        print('minimum query before threshold ('+str(threshold*100)+'%) :'+str(min_wanted_query)+' ('+str(q_ratio)+'% of current pool size)')\n",
    "        if(len(learning_rate)!=0):\n",
    "            print(' average learning rate : '+ str(np.average(learning_rate)))\n",
    "            learning_distrib.append(np.average(learning_rate))\n",
    "        else :\n",
    "            learning_distrib.append(0)\n",
    "            \n",
    "        if(nb_quarter !=0):\n",
    "            unc_evolution.append(classifier_uncertainty(current_learner, measurement[1]))\n",
    "            print('measurement sample average uncertainty :' + str(np.average(unc_evolution[-1])))\n",
    "        \n",
    "        if(F1<threshold):\n",
    "        #    current_learner = prev_learner\n",
    "            print('threshold not reached, model NOT rollback')\n",
    "            \n",
    "        \n",
    "        next_it = current_q + 6*(current_y-2009)\n",
    "        unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it]\n",
    "        current_pool_size = len(unlabeled_y)\n",
    "\n",
    "\n",
    "        MAX_QUERY = current_pool_size\n",
    "        print('transition F1 :' + str(calculate_acc(current_learner, test_x, test_y)[3]))\n",
    "        print('-----------------------------------------------------------------')\n",
    "        \n",
    "        \n",
    "    \n",
    "    return acc_list, F_list, required_querry, unc_evolution, uncertainty_distrib, acc_ref, F_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_list, F_list, MQ_list, unc_evolution, distrib, acc_ref, F_ref = learning_experiment5(2011, 6, 40 , quarter_list_hyb, 'F1', 'over', threshold = 0.98)\n",
    "analysis(acc_list, F_list, MQ_list, [], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [np.average(e) for e in unc_evolution]\n",
    "ind = [i for i in range(len(temp))]\n",
    "plt.plot(ind, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(unc_evolution)\n",
    "ind = [i for i in range(len(unc_evolution))]\n",
    "#plt.plot(ind, unc_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MQ_ind1= []\n",
    "summ=0\n",
    "for e in MQ_list:\n",
    "    summ+=e[0]\n",
    "    MQ_ind1.append(summ)\n",
    "acc=[acc_ref[i] for i in MQ_ind1]\n",
    "acc2=[acc_list[i] for i in MQ_ind1]\n",
    "F=[F_ref[i] for i in MQ_ind1]\n",
    "F2=[F_list[i] for i in MQ_ind1]\n",
    "\n",
    "ind = [i for i in range(len(acc))]\n",
    "\n",
    "plt.plot(ind, acc2, label ='acc')\n",
    "plt.plot(ind, acc, label = 'ref acc')\n",
    "\n",
    "plt.plot(ind, F2, label ='F1')\n",
    "plt.plot(ind, F, label ='F1 ref')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MQ_ind1= []\n",
    "summ=0\n",
    "for e in MQ_list:\n",
    "    summ+=e[0]\n",
    "    MQ_ind1.append(summ)\n",
    "F=[F_ref[i] for i in MQ_ind1]\n",
    "F2=[F_list[i] for i in MQ_ind1]\n",
    "\n",
    "ind = [i for i in range(len(F))]\n",
    "ind2 = [i for i in range(len(F2))]\n",
    "\n",
    "plt.plot(ind, F)\n",
    "plt.plot(ind, F2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TEST_PERIOD = 44\n",
    "q1 = 5\n",
    "y1 = 2011\n",
    "\n",
    "#uncertainty evolution measurement\n",
    "\n",
    "def learning_experiment6(y1, q1, nb_q, data, mode, balancing, threshold=0.98, nb_query=200, show_query=False, show_it=False, show_balance=True, random = False):\n",
    "    #creation of the initial pools\n",
    "    \n",
    "    next_it = q1 + 6*(y1-2009)\n",
    "    unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it-1]\n",
    "    X_training, y_training = init_balanced_training_pool(unlabeled_x,unlabeled_y, 10)  \n",
    "    \n",
    "    if (balancing == 'under'):\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        unlabeled_x, unlabeled_y = undersample.fit_resample(unlabeled_x, unlabeled_y)\n",
    "    elif(balancing == 'over'):\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "        unlabeled_x, unlabeled_y = oversample.fit_resample(unlabeled_x, unlabeled_y)       \n",
    "    else :\n",
    "        print('error : invalid balancing method')\n",
    "        return 0,0,0,0\n",
    "    current_pool_size = len(unlabeled_y)\n",
    "    MAX_QUERY = current_pool_size\n",
    "    \n",
    "    \n",
    "    #creation of the initial model\n",
    "    if(random):\n",
    "        current_learner = ActiveLearner(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        query_strategy=rd_sampling,\n",
    "        X_training=X_training, y_training=y_training\n",
    "        )\n",
    "    else :\n",
    "        current_learner = ActiveLearner(\n",
    "            estimator=RandomForestClassifier(),\n",
    "            query_strategy=uncertainty_sampling,\n",
    "            X_training=X_training, y_training=y_training\n",
    "        )\n",
    "    #reference supervised model we want to measure the evolution of\n",
    "    ref_supervised_learner = RandomForestClassifier()\n",
    "    \n",
    "    acc_list = []\n",
    "    F_list = []\n",
    "    it=[]\n",
    "    required_querry=[]\n",
    "    autolabeled = []\n",
    "    uncertainty_distrib = []\n",
    "    learning_distrib = []\n",
    "    \n",
    "    acc_ref =[]\n",
    "    F_ref = []\n",
    "    \n",
    "    ref_supervised_learner.fit(unlabeled_x, unlabeled_y)\n",
    "    acc, recall, precision, F = calculate_acc(ref_supervised_learner, test_x, test_y)\n",
    "    acc_ref.append(acc)\n",
    "    F_ref.append(F)\n",
    "    accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "    acc_list.append(accuracy)\n",
    "    F_list.append(F1)\n",
    "    \n",
    "    unc_evolution = []\n",
    "    measure_acquired = False\n",
    "    \n",
    "    for nb_quarter in range(nb_q):\n",
    "        nb_request=0\n",
    "        nb_complement_label=0\n",
    "        nb_false_pos = 0\n",
    "        nb_false_neg = 0\n",
    "        condition = True\n",
    "        min_wanted_query = 0\n",
    "        prev_learner = current_learner\n",
    "        local_distrib =[]\n",
    "        \n",
    "        if(show_balance):\n",
    "            p=see_balance(unlabeled_y, 'total pool')\n",
    "            \n",
    "        learning_rate = []\n",
    "            \n",
    "        while(condition):\n",
    "            \n",
    "            predict = classifier_uncertainty(current_learner, unlabeled_x)\n",
    "            av_unc = np.average(predict)\n",
    "            local_distrib.append(predict)\n",
    "            if(nb_request==0):\n",
    "                print('average starting uncertainty :' + str(av_unc))\n",
    "            \n",
    "            ##second query/auto labeling process :\n",
    "            #only does auto lbeling once, then keep track of th auto labelled element to see how their uncertainty evolve\n",
    "            \n",
    "            \n",
    "            if(not nb_quarter==0 and measure_acquired==False):\n",
    "                query_idx_list, query_inst_list, label_list = second_query(current_learner, unlabeled_x, threshold_const, 1)\n",
    "                measurement = [query_idx_list, query_inst_list, label_list]\n",
    "                measure_acquired = True\n",
    "            \n",
    "            query_idx_list = np.array([],dtype=int)\n",
    "            query_inst_list = np.array([],dtype=int)\n",
    "            label_list = np.array([],dtype=int)\n",
    "\n",
    "\n",
    "            \n",
    "                \n",
    "            ##standart querry process :\n",
    "            query_idx, query_inst = current_learner.query(unlabeled_x)\n",
    "            nb_request +=1\n",
    "            \n",
    "            if(query_inst_list.size>0):\n",
    "                query_idx_list = np.concatenate((query_idx_list, query_idx), axis=0)\n",
    "                query_inst_list = np.concatenate((query_inst_list, query_inst), axis=0)\n",
    "                label_list=np.concatenate((label_list, unlabeled_y[query_idx]), axis=0)\n",
    "            else :\n",
    "                query_idx_list = query_idx\n",
    "                query_inst_list = query_inst\n",
    "                label_list = unlabeled_y[query_idx]\n",
    "            \n",
    "\n",
    "            current_learner.teach(X=query_inst_list,\n",
    "                                          y=label_list)\n",
    "            \n",
    "            for i,e in enumerate(label_list):\n",
    "                if(unlabeled_y[query_idx_list[i]] == 1 and e == 0):\n",
    "                    nb_false_pos+=1\n",
    "                elif(unlabeled_y[query_idx_list[i]] == 0 and e == 1):\n",
    "                    nb_false_neg+=1\n",
    "                \n",
    "            unlabeled_x = np.delete(unlabeled_x, query_idx_list, axis=0)\n",
    "            unlabeled_y = np.delete(unlabeled_y, query_idx_list)\n",
    "            \n",
    "            acc, recall, precision, F = calculate_acc(ref_supervised_learner, test_x, test_y)\n",
    "            acc_ref.append(acc)\n",
    "            F_ref.append(F)\n",
    "            \n",
    "            accuracy, recall, precision, F1 = calculate_acc(current_learner, test_x, test_y)\n",
    "            \n",
    "            \n",
    "            acc_list.append(accuracy)\n",
    "            F_list.append(F1)\n",
    "            \n",
    "            if(nb_request>1):\n",
    "                learning_rate.append(F_list[-1]-F_list[-2])\n",
    "            \n",
    "            if(accuracy>=threshold and min_wanted_query<1):\n",
    "                min_wanted_query = nb_request\n",
    "            if(show_it and (100*nb_request/nb_query)%10==0):\n",
    "                print(str(int(100*nb_request/nb_query))+'%')\n",
    "                \n",
    "            if(mode == 'query'):\n",
    "                condition = nb_request < nb_query\n",
    "            elif(mode == 'acc'):\n",
    "                condition = (accuracy < threshold) & (len(unlabeled_y) > 0)\n",
    "            elif(mode == 'F1'):\n",
    "                condition = (F1 < threshold) & (len(unlabeled_y) > 0)    \n",
    "            else :\n",
    "                print('error : invalid mode')\n",
    "                return 0,0,0,0,0\n",
    "\n",
    "        uncertainty_distrib.append(local_distrib)\n",
    "        if(min_wanted_query<1):\n",
    "            min_wanted_query = nb_request\n",
    "        print('final F1 at q'+str(nb_quarter)+' ('+str(current_q)+'/'+str(current_y)+'): '+str(F1))\n",
    "        q_ratio = 100*min_wanted_query/current_pool_size\n",
    "        required_querry.append((min_wanted_query,q_ratio))\n",
    "        autolabeled.append((nb_complement_label, (nb_false_pos, nb_false_neg), av_unc))\n",
    "        print('minimum query before threshold ('+str(threshold*100)+'%) :'+str(min_wanted_query)+' ('+str(q_ratio)+'% of current pool size)')\n",
    "        if(len(learning_rate)!=0):\n",
    "            print(' average learning rate : '+ str(np.average(learning_rate)))\n",
    "            learning_distrib.append(np.average(learning_rate))\n",
    "        else :\n",
    "            learning_distrib.append(0)\n",
    "            \n",
    "        if(nb_quarter !=0):\n",
    "            unc_evolution.append(classifier_uncertainty(current_learner, measurement[1]))\n",
    "            print('measurement sample average uncertainty :' + str(np.average(unc_evolution[-1])))\n",
    "        \n",
    "        if(F1<threshold):\n",
    "        #    current_learner = prev_learner\n",
    "            print('threshold not reached, model NOT rollback')\n",
    "            \n",
    "        \n",
    "        next_it = current_q + 6*(current_y-2009)\n",
    "        unlabeled_x, test_x,unlabeled_y, test_y, current_q, current_y = data[next_it]\n",
    "        current_pool_size = len(unlabeled_y)\n",
    "\n",
    "\n",
    "        MAX_QUERY = current_pool_size\n",
    "        print('transition F1 :' + str(calculate_acc(current_learner, test_x, test_y)[3]))\n",
    "        print('-----------------------------------------------------------------')\n",
    "        \n",
    "        \n",
    "    \n",
    "    return acc_list, F_list, required_querry, unc_evolution, uncertainty_distrib, acc_ref, F_ref"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
